apiVersion: v1
kind: Pod
metadata:
  name: embedding-models
spec:
  containers:
  - name: triton-inference-server
    securityContext:
      privileged: true
    ports:
     - containerPort: 8000
       name: http-triton
     - containerPort: 8001
       name: grpc-triton
     - containerPort: 8002
       name: metrics-triton
    image: "espymur/triton-onnx-cpu:dev"
    volumeMounts:
     - mountPath:  /var/s3fs:shared
       name: oracle-cloud-fs
    command: ["/bin/sh", "-c"]
    args: ["tritonserver --model-repository=/var/s3fs:shared"] #  put the model in a cloud storage and pull it from here.
    resources:
      requests:
        cpu: 2
        memory: 4Gi
  volumes:
    - name: devfuse
      hostPath:
        path: /dev/fuse
    - name: oracle-cloud-fs
      hostPath:
        path: /var/s3


