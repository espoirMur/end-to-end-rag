### Deploy an Embedding Model using ONNX 


### Layout

- overview of embeddings models.
- Example using embedding model from huggingface
- Converting the model to ONNX
- 

## Embeddings.

Embedding models, Open AI, if you are in tech you should embedding model should be familiar to you. 

Embedding models are the backbone and one of major piece of a generative AI applications. 
Embedding are representation of words in a vector space such as similar words have similars vectors.  Contextual embeddings are embedding such as each word is represent with a vector given it context. 
Let look at those two example 

`The bank of the river thames is located in south london`

`I am going to withdraw cash at Lloyds bank`

In those two different sentences the word `bank` have two different meaning, in the first bank it mean a _the land alongside or sloping down to a river or lake_, in the second sentence it means a place where you save money.

Embedding models are able to capture those difference and represent words with two different vectors according to the context.

This is not a post to explain how embedding models are build, if you want to learn more about them refers to this post:

- [Put the financial post.]

But one thing to know that embedding models are build with language models or Large language models.


## Large Language Models.

Large language model are neural networks or probabilistic models that can predict the next word given the previous words. One of the most common neural network architecture to power language models are the transformer models which was introduced in 2017 by google researchers. Those models have powerful capacity when to come to understand words and their meanings because they are trained on large corpus of documents.

During their training transformers model are able to learn contextual word embedding and those embedding are useful in downstream application such chatbot, document classification, topic modeling, document clustering  et consort.. .

Again, this post is not about language models, there legion on the internet, my favourite one is the {illustrated transformer by Jamal}

If this post is not about word embeddings theory, nor large language model theory what is it about?

Nice question, this post is about deploying a large language model and create an embedding service, a api that developers can query to generate document embedding. Basically we will build a scalable api that developers can query to retrieve words embeddings of their sentences, and use them in dowstream application. This api can be part of a chatbot, or a Retrieval Augmented Generation application.

In the post we will learn how to deploy a transformer model that generate embedding vectors on Kubernetes using the triton inference server and the ONNX runtime.

Enough talking let show us the code

[talk about the post layout.]

## The embedding models.

In this post we will explore embedding model generated by the BioLinkBert models: the bioLinkBert model is a model from the BERT family but it was fined tuned on documents from the medical domain. The reason I used the biolink model is because I want to build a chatbot application for medical domain in the future.

The embedding of words are the last hidden state of a transformer model where the output is the word encoded as text. Let us see how it work  in practice. we will be using a custom bert model which inherit the base bert model from hugging face.

```python

import torch
from dataclasses import dataclass
from typing import Optional, Mapping, OrderedDict
from transformers.onnx import OnnxConfig
from transformers.utils import ModelOutput

@dataclass
class EmbeddingOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = None


class CustomEmbeddingBertModel(BertModel):
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
    ) -> EmbeddingOutput:
        embeddings = super().forward(input_ids=input_ids,
                                     attention_mask=attention_mask,
                                     head_mask=head_mask,
                                     inputs_embeds=inputs_embeds,
                                     output_attentions=True,
                                     output_hidden_states=True,
                                     return_dict=True)
        mean_embedding = embeddings.last_hidden_state.mean(dim=1)
        embedding_output = EmbeddingOutput(last_hidden_state=mean_embedding)

        embedding_output = embedding_output.last_hidden_state.detach().numpy().reshape(-1)
        return embedding_output
```

Our custom embedding is just a wrapper around the bert embedding which take the input ids and return the embedding of a sentence. The input ids is the tokenized version of a sentence, the embeddings of the sentence is the average of the embedding of all words in a  sentence.

Here is how that work in practice.


```python

embedding_model_id = 'michiyasunaga/BioLinkBERT-large'
base_model = CustomEmbeddingBertModel.from_pretrained(embedding_model_id)


```


Before passing the text to the embedding the text need to be transformed in a tokenizer, 

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(embedding_model_id)
```


```python

test_input = f"what is the cause of Covid"
encoded_input = tokenizer([test_input],
                          return_tensors='pt',
                          max_length=512,
                          truncation=True)

```


With our encoded_input and the base model we can generate the text embedding for our text input.

```python
text_embeddings = base_model(**encoded_input)
print(text_embeddings.shape)
```

The text embedding is the embedding representation of the sentence in text_input.
It can be use in downstream application in different ways.

The next step is save the model in the format we can use to deploy it in production.


## Exporting the Model to Onnx format

### What is the ONNX format?

ONNX stands for Open Neural Network Exchange. It is an open format built to represent machine learning models in a framework-agnostic way.

As you may know, neural networks are computation graphs with input, weights, and operations. [Cite the source here.]

ONNX format is a way of saving neural networks as computation graphs. That  computational graph represents the flow of data through the neural network.


The key benefits of saving neural networks in the ONNX format are interoperability and hardware access. Any deep learning platform can read a neural network saved in the ONNX format.  For example, a model trained in Pytorch can be exported to ONNX format and imported in Tensorflow and vice versa.

You don't need to use Python to read a model saved as ONNX; you can use any programming language of your choice, such as Javascript, C, or C++. 

ONNX makes the model easier to access hardware optimizations, and you can apply other optimizations, such as quantization, to your ONNX model.

Let us see how we can convert our model to ONNX format to use the full benefits of it.


Let see how we can achieve that with the code.

```python
from pathlib import Path
model_path = Path.cwd()
model_path = model_path.joinpath("onnx")
model_path.mkdir(exist_ok=True, parents=True)
```


```python

from torch.onnx import export as torch_onnx_export

torch_onnx_export(
    base_model,
    tuple(encoded_input.values()),
    f=model_path.joinpath('bio-bert-embedder.onnx'),
    input_names=['input_ids', 'attention_mask'],
    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},
                  'attention_mask': {0: 'batch_size', 1: 'sequence'},
                  'last_hidden_state': {0: 'batch_size', 1: 'sequence'}},
    do_constant_folding=True,
    opset_version=13,
)
```


With the above code, we have our model exported into onnx format and ready to be deployed in production.



# Part 2 Deploy the model with docker to the ONNX runtime

In this section we will learn how  do use the transformed model in docker container.

One of the most obvious solution is to deploy the model and wrap it in a flask or fastapi, which will work and produce some results. However for this blog I will try a different approach, I will deploy the model using the onnx runtime which is a c++ backend. We will leverage the fact that our model in onnx format is platform agnostic and we can deploy on any language backend.


## Triton Server

Triton is a software tool for deploying machine learning models for inference. It is designed to produce high-quality inference across different hardware platforms, either GPU or CPU. It also supports inference across cloud, data center, and embedded devices.
One of the advantages of the triton server is that it supports dynamic batching and concurrent model execution.

- Dynamic batching, for models that support batching, which is the case for deep learning models, triton implements scheduling and batching algorithms that combine individual requests to improve inference throughput.

    
- Concurrency model execution is the capacity to run simultaneously multiple models on the same GPU or various GPUs.


### Triton Server Backend

Triton supports different backends to execute the model. A backend is a wrapper around a deep learning framework like Pytorch, TensorFlow, TensorRT, or ONNX Runtime.
Two backend types interested us for this post: the Python Backend and the ONNX runtime backend. 

The ONNX runtime backend executes ONNX models, and the Python backend allows the writing of the model logic in Python. 

In this post, we will be focused on the ONNX and the Python backend.


### The Triton Server

Let us setup the model repository for the triton inference server.


```bash
#!/bin/bash

mkdir -p models_repository/embedding_models/embedding_model/1
touch models_repository/embedding_models/embedding_model/config.pbtxt


mkdir -p models_repository/embedding_models/ensemble_model/1
touch models_repository/embedding_models/ensemble_model/config.pbtxt

touch models_repository/embedding_models/tokenizer/1/model.py

touch models_repository/embedding_models/tokenizer/config.pbtxt

```


This bash script will create the model repository  for our embedding model, in the next section we will setup the files in that model repository in order to run our models.



The model repository should have 3 components, the tokenizer, the embedding model, and the ensemble model.

The tokenizer is the configuration of our tokenizer model, it use the python backend and handle tokenization of our text input.


The tokenizer repository should have the files from our tokenizer, the model code, and the model configuration.
It should have the following layout
```
└── tokenizer
    ├── 1
    │   ├── __pycache__
    │   ├── config.json
    │   ├── model.py
    │   ├── special_tokens_map.json
    │   ├── tokenizer.json
    │   ├── tokenizer_config.json
    │   └── vocab.txt
    └── config.pbtxt
```
To create the tokenizer file, we will have to save our tokenizer to the tokenizer repository, we will use the following code.


```python
model_repository = Path.cwd().joinpath("model_repository")

tokenizer_path = model_repository.joinpath("retrieval", "tokenizer")
tokenizer_path = tokenizer_path.joinpath("1")
tokenizer.save_pretrained(tokenizer_path)

```


From that tokenizer we will create the model.py file, which will handle the tokeinization part.


Here is how the model should look like

```python

import os
from typing import Dict, List

import numpy as np
import triton_python_backend_utils as pb_utils
from transformers import AutoTokenizer, PreTrainedTokenizer, TensorType


class TritonPythonModel:
    tokenizer: PreTrainedTokenizer

    def initialize(self, args: Dict[str, str]) -> None:
        """
        Initialize the tokenization process
        :param args: arguments from Triton config file
        """
        # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc
        path: str = os.path.join(
            args["model_repository"], args["model_version"])
        self.tokenizer = AutoTokenizer.from_pretrained(path)

    def execute(self, requests) -> "List[List[pb_utils.Tensor]]":
        """
        Parse and tokenize each request
        :param requests: 1 or more requests received by Triton server.
        :return: text as input tensors
        """
        responses = []
        # for loop for batch requests (disabled in our case)
        for request in requests:
            # binary data typed back to string
            query = [
                t.decode("UTF-8")
                for t in pb_utils.get_input_tensor_by_name(request, "TEXT")
                .as_numpy()
                .tolist()
            ]
            tokens: Dict[str, np.ndarray] = self.tokenizer(
                text=query, return_tensors=TensorType.NUMPY, padding=True, truncation=True
            )
            # tensorrt uses int32 as input type, ort uses int64
            tokens = {k: v.astype(np.int64) for k, v in tokens.items()}
            # communicate the tokenization results to Triton server
            outputs = list()
            for input_name in self.tokenizer.model_input_names:
                tensor_input = pb_utils.Tensor(input_name, tokens[input_name])
                outputs.append(tensor_input)

            inference_response = pb_utils.InferenceResponse(
                output_tensors=outputs)
            responses.append(inference_response)

        return responses

```


The `initialize` method from this class will create our tokenizer from this folder, as it contains all our tokenizer files it will be initialize from it.


The execute method is the one that handle the request, it can take multiple requests and parse the request, and create the query query from the text, and finally return the tokenized text.

With our tokenizer setup, let us configure the python server to use it.

The content of the tokenizer/config.pbxt should loook like this.

```bash

name: "tokenizer"
max_batch_size: 0
backend: "python"

input [
{
    name: "TEXT"
    data_type: TYPE_STRING
    dims: [ -1 ]
}
]

output [
{
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [-1, -1]
},
{
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [-1, -1]
}
]

instance_group [
    {
      count: 1
      kind: KIND_CPU
    }
]
```

In this file, we specify that our backend is a python backend and it will take an imput name text, with dimension -1, which mean dynamic, and return the inputs_ids, and the attention_mask and it will run on a cpu.



The second component of our model is the embedding model itself, we will it should have the following layout.

├── embedding_model
│   ├── 1
│   │   ├── bio-bert-embedder.onnx
│   │   └── config.json
│   └── config.pbtxt



The file named `bio-bert-embedder.onnx` it the onnx version of our embedding model, we should copy the bio-bert-embedder model here.

Let look at the `config.pbtxt`

```
name: "embedding_model"
platform: "onnxruntime_onnx"
backend: "onnxruntime"
default_model_filename: "bio-bert-embedder.onnx"
max_batch_size: 0
input [
  {
    name: "input_ids"
    data_type: TYPE_INT64
    dims: [ -1, -1 ]
  },
{
    name: "attention_mask"
    data_type: TYPE_INT64
    dims: [ -1, -1 ]
  }
]
output [
  {
    name: "3391"  # not sure why this is name 3391, need to double check
    data_type: TYPE_FP32
    dims: [ -1, 1024 ]
  }
]

instance_group [
    {
      count: 1
      kind: KIND_CPU
    }
]
```

it is the configuration file for our embedding model, we can see that it takes the output from our tokenizer model and produce the embedding vector of shape, -1, 1024. With -1 meaning the dynamic shape, and 1024 is our embedding size.


Note: for some reason the model output is name `3391` i  don't know why it is named like that.

With our embedding model and the tokenizer we can connect their input and output with the ensemble model. It should have the following layout


```
├── ensemble_model
│   ├── 1
│   └── config.pbtxt
```

And the content of the `config.pbtxt` file should be like this:


```

name: "ensemble_model"
# maximum batch size 
max_batch_size: 0 
platform: "ensemble"

#input to the model 
input [
{
    name: "TEXT"
    data_type: TYPE_STRING
    dims: [ -1 ] 
    # -1 means dynamic axis, aka this dimension may change 
}
]

#output of the model 
output {
    name: "3391"
    data_type: TYPE_FP32
    dims: [-1, 1024] 
    # two dimensional tensor, where 1st dimension: batch-size, 2nd dimension: #classes, not sure why its 3391.
}

#Type of scheduler to be used
ensemble_scheduling {
    step [
        {
            model_name: "tokenizer"
            model_version: -1
            input_map {
            key: "TEXT"
            value: "TEXT"
        }
        output_map [
        {
            key: "input_ids"
            value: "input_ids"
        },
        {
            key: "attention_mask"
            value: "attention_mask"
        }
        ]
        },
        {
            model_name: "embedding_model"
            model_version: -1
        input_map [
            {
                key: "input_ids"
                value: "input_ids"
            },
            {
                key: "attention_mask"
                value: "attention_mask"
            }
        ]
        output_map {
                key: "3391"
                value: "3391"
            }
        }
    ]
}

```

In a nutshell this config connect our tokenizer and the embedding model, you can easily see that from it the output of the tokenizer model are passed to the embedding model to produce the embedding vector.


If the three component where configure correctly we can have the following layout:

```

models_repository/retrieval
├── embedding_model
│   ├── 1
│   │   ├── bio-bert-embedder.onnx
│   │   └── config.json
│   └── config.pbtxt
├── ensemble_model
│   ├── 1
│   └── config.pbtxt
└── tokenizer
    ├── 1
    │   ├── __pycache__
    │   ├── config.json
    │   ├── model.py
    │   ├── special_tokens_map.json
    │   ├── tokenizer.json
    │   ├── tokenizer_config.json
    │   └── vocab.txt
    └── config.pbtxt

```


If you have all the following component we can go to the next stage.


### Building the triton Inference server image.


In this section we will see how to build the triton inference server image. The base triton inference server docker image is huge and can weight up to 10 GB. In the triton inference server there is a way to build a cpu only image for triton, but I wasn't able to build it from my macbook, luckily, `jackiexiao/tritonserver:23.12-onnx-py-cpu`  have the image and pushed it to the repository, we will use it to build ours.


Here is the docker file use do build this image.

```
# %load Dockerfile
# Use the base image
FROM jackiexiao/tritonserver:23.12-onnx-py-cpu



# Install the required Python packages
RUN pip install transformers==4.27.1 sacremoses==0.1.1



```

You can see that we are just pulling the base image and install in it the transformer and the moses tokenizer.

With that docker image we can build the docker image.

` docker build -t espymur/triton-onnx-cpu:dev  -f Dockerfile .`



And if the image was succefully build we push to the docker image repository:

`docker push espymur/triton-onnx-cpu:dev`

If the image was successfully build you can start your docker container with the triton server in it.


```
 docker run --rm -p 8000:8000 -p 8001:8001 -p 8002:8002  -v ${PWD}/models_repository/retrieval:/models  espymur/triton-onnx-cpu:dev tritonserver --model-repository=/models
```


This command start a docker container, and expose different port to the exterior environment, such as the port 8000 for http, the port 8001 for grpc and the port 8002 for metric server, after that we use the volume `-v` to mount  the model_repository folder which contain our models to the /model folder in docker container, we specify our image, the name of the container and finally we specify the path to the model_repository in the container.

If everything goes well in with that command you should be able to see following output which tell us which port is used by the model.

```

I0329 18:42:18.452806 1 grpc_server.cc:2495] Started GRPCInferenceService at 0.0.0.0:8001
I0329 18:42:18.460674 1 http_server.cc:4619] Started HTTPService at 0.0.0.0:8000
I0329 18:42:18.520315 1 http_server.cc:282] Started Metrics Service at 0.0.0.0:8002
```

With that code we have our embedding api running and we can now send requests to it.

### Making Request to the inference Server.

We have now built our model, the next step is now to make inference request to it and analyze the response.

Since the model is deployed as a rest api you can make inference requests to it using any client of your choice in any language.  The inference server is very strict in terms of what it expect as input, and how to build it, fortunately they have describe different client to use to build the inputs. For demonstration purposes I will be using the python http client to make inference requests.

```python
import numpy as np
import tritonclient.http as httpclient
url = localhost:8000
http_client = httpclient.InferenceServerClient(url=url,
                                               verbose=True)
```


The above code creates the http client, with our server url, let us define the input and output of it.


```python
text_input = httpclient.InferInput('TEXT', shape=[-1], datatype='BYTES')

embedding_output = httpclient.InferRequestedOutput("3391", binary_data=False

```

Those are the placeholder for our inputs and output, let us fill them now:

```python
sentences = ["what cause covid"]
np_input_data = np.asarray([sentences], dtype=object)
text_input.set_data_from_numpy(np_input_data.reshape([1]))
```


With the input placeholder and the outputs, we can send our request to the server:

```python
results = http_client.infer(model_name="ensemble_model", inputs=[text_input], outputs=[embedding_output])
```


We can now convert back the output to numpy using


```
inference_output = results.as_numpy('3391')
print(inference_output.shape)

```


That is all we have our embedding api, that takes the text and produce the embedding vector.

### Conclusion

In this post, we have learned how to deploy an embedding model as an api using the triton inference server. The knowledge learned in this post can be used to deploy any transformer model  with an encoder or decoder using the triton inference server. Any model from the BERT, or GPT family.  It can slightly  be adapted to use with encoder - decoder models such as T5 or M2M.

Once we deploy the model to production server it will grow with users and need to scale, in the next section we will learn how to scale the model using Kubernetes.


