version: '3.7'
services: 
    llama-server:
        image: ghcr.io/espoirmur/llama.cpp:server--b1-8c5997d
        restart: always
        ports: 
            - "8001:8001"
        volumes:
          - /var/s3:/models
        command: -m /models/croissantllm32.gguf --port 8001 -n 256
    storage-mount:
        build: 
            context: ./storage/
            dockerfile: DockerFile
        security_opt:
          - apparmor:unconfined
        cap_add:
          - SYS_ADMIN
        devices:
          - /dev/fuse
        env_file:
          .env
volumes:
  model_repository:
