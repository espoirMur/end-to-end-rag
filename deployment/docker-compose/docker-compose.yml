services: 
    llama-server:
        image: ghcr.io/espoirmur/llama.cpp:server--b1-8c5997d
        restart: always
        ports: 
            - "8001:8001"
        volumes:
          - model_repository:/models
        command: -m /models/croissantllm_32.16bits.gguf --port 8000 -n 256 --host 0.0.0.0
volumes:
  model_repository:
    driver: s3fs
    name: "croissant-llm-model"
