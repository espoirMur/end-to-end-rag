{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = rg.Argilla(\n",
    "    api_url = 'http://localhost:3000',\n",
    "    api_key='argilla.apikey',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model_id = \"JDBN/t5-base-fr-qg-fquad\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    generative_model_id, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(generative_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "random_id = randint(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.database import execute_query, generate_database_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = generate_database_connection()\n",
    "random_article = execute_query(\n",
    "    database_connection, f'SELECT content FROM article WHERE id = {random_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article = random_article[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"generate question : {random_article}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer([model_input],\n",
    "                              return_tensors='pt').to(model.device)\n",
    "output = model.generate(input_ids=encoded_input.input_ids,\n",
    "                        do_sample=True, \n",
    "                            top_k=5, \n",
    "                            temperature=0.6,\n",
    "                            max_length=256)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18d83a8abac44d29222496e4eaabb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/19.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6780777fa75b4f83b9bd4d2459727c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750f03e3c39e4eb5b8ec4f3a1d577455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fefadb43f44de3a7a3fee13b402697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/732 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/mambaforge/envs/env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f155161c07a445ff8acf29e33b76f9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8051ee231abf4d03b10e7c3abfe9b1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f566f4627fa94c238926e389b73afa4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a77cd4ad6c4c5089df5835add3ff03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/397M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59a00133667480e87937ce8d47e21a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e58947f78c4455dbf99b0be6de3e7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"croissantllm/CroissantLLMChat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"generate 5 french questions  and their answers based on the following text : {random_article}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.90,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "}\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": f\"{model_input}\"},\n",
    "]\n",
    "\n",
    "chat_input = tokenizer.apply_chat_template(\n",
    "    chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "tokens = model.generate(**inputs, **generation_args)\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a good french model that can be run properly on CPU. It slow, but it works. Let now use this model to generate 5k questions that can be answered with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text):\n",
    "    \"\"\" Split the text inside the  <|im_start|> assistant <|im_end|> tags and then split the new line text  and return the pair question and response\"\"\"\n",
    "   \n",
    "    # use regex to get the text inside the <|im_start|> assistant <|im_end|> tags\n",
    "    text = text.split(\"<|im_start|> assistant\")[1].split(\"<|im_end|>\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses  = parse_response(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_questions_and_responses(text):\n",
    "    # Define a regex pattern to match questions and their responses\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    pattern = re.compile(\n",
    "        r'(\\d+\\.\\s[^?]+?\\?)\\sRéponse:\\s([^0-9]+?)(?=\\d+\\.|$)', re.DOTALL)\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    # Create a list of tuples with (question, response)\n",
    "    result = [(match[0].strip(), match[1].strip()) for match in matches]\n",
    "    result = [\n",
    "        f\"Question: {question_answer[0]} \\n Réponse: {question_answer[1]}\" for question_answer in result]\n",
    "    return  \"\\n\\n\".join(result)\n",
    "\n",
    "\n",
    "# Extract the questions and responses\n",
    "questions_and_responses = extract_questions_and_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_text = rg.TextField(name=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_question = rg.TextQuestion(\n",
    "    name=\"text\",\n",
    "    title=\"Are this the correct question_answer pairs based on the questions\",\n",
    "    description=\"Please provide feedback on the response\",\n",
    "    required=True,\n",
    "    use_markdown=True\n",
    ")\n",
    "\n",
    "answers = rg.TextField(name=\"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_setting = rg.Settings(\n",
    "    guidelines=\"Please provide feedback on the response\",\n",
    "    fields=[paragraph_text, answers],\n",
    "    questions = [annotation_question]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset = rg.Dataset(\n",
    "    name=\"annotation_dataset\",\n",
    "    settings=annotation_setting,\n",
    "    client=client,\n",
    "    workspace = \"argilla\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = client.workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = rg.Record(\n",
    "    fields={\"content\": random_article, \"answers\": questions_and_responses},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset.records.log([record])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_template(content):\n",
    "  prompt = f\"generate 5 french questions , and for each question one answer in the format (question, answer) based on the following text : {content}\"\n",
    "  chat = [\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "  chat_input = tokenizer.apply_chat_template(\n",
    "      chat, tokenize=False, add_generation_prompt=True)\n",
    "  return chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = Path().cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_path = current_directory.joinpath(\"subset_to_label.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 06:29:32.131545: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-17 06:29:32.248679: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-17 06:29:32.248708: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-17 06:29:32.248715: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-17 06:29:32.302087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd13b8f61fa496792e0f3575d93d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files={'train': subset_path.__str__()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfc8df5c35c40548d08c2f4a5eae8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "                      \"prompt_template\": generate_prompt_template(x[\"content\"])}, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = current_directory.joinpath(\"datasets\", \"congo_news_qa\")\n",
    "dataset_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def write_list_to_json_file(list_of_strings, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(list_of_strings, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_and_answers(examples):\n",
    "    \"\"\"\n",
    "    take a batch of example compute the embeddings and save the subset of the embeddings\n",
    "    Add a new columns named embedding to the subsets of example and save the subset locally.\n",
    "    \"\"\"\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.90,\n",
    "        \"top_k\": 40,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "    }\n",
    "\n",
    "    inputs = tokenizer(examples[\"prompt_template\"],\n",
    "                       return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, **generation_args)\n",
    "    questions_and_answers = tokenizer.batch_decode(\n",
    "        outputs, skip_special_tokens=True)\n",
    "    examples[\"question_answers\"] = questions_and_answers\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_shards = 1000 // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_of_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd196345c2740fdada31afeafced130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b601a07ea9d4320853027b4b392d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ddb1439e124f1e99ddb56a90e7e500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51429ed195e3423ca7322f263d54233e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be0b5866f4148fc97e1b208b00977b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3630d9dfcdc4dc290b09d928fb79703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2911a9acc0749178f5bd76e53e7ffb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1b6f493aab4288ba4ba2d94ab8ce1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6e915b5a414a80aa722ae6c3b6cbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1437e0981e5e4ad68c09ea8b2b56e156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084006db486a432aaf7900169ccbcdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa24cf32860340c2b00069a9536b6168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597268cc3e0f4180aaa881ec64778e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cb97b9cfb34ae0a6a30f66512acc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing shard  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1efb6723e44c53b81a93004e7b565e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for shard_index in range(0, numbers_of_shards):\n",
    "    shard = dataset[\"train\"].shard(num_shards=numbers_of_shards, index=shard_index)\n",
    "    shard = shard.map(extract_question_and_answers, batched=True, batch_size=2)\n",
    "    shard.save_to_disk(dataset_path.joinpath(f\"shard_{shard_index}\"))\n",
    "    print(\"done processing shard \", shard_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
