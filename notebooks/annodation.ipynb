{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = rg.Argilla(\n",
    "    api_url = 'http://localhost:3000',\n",
    "    api_key='argilla.apikey',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model_id = \"JDBN/t5-base-fr-qg-fquad\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    generative_model_id, device_map=\"auto\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(generative_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "random_id = randint(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.database import execute_query, generate_database_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = generate_database_connection()\n",
    "random_article = execute_query(\n",
    "    database_connection, f'SELECT content FROM article WHERE id = {random_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article = random_article[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"generate question : {random_article}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer([model_input],\n",
    "                              return_tensors='pt').to(model.device)\n",
    "output = model.generate(input_ids=encoded_input.input_ids,\n",
    "                        do_sample=True, \n",
    "                            top_k=5, \n",
    "                            temperature=0.6,\n",
    "                            max_length=256)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"croissantllm/CroissantLLMChat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"generate 5 french questions  and their answers based on the following text : {random_article}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.90,\n",
    "    \"top_k\": 40,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "    \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "}\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": f\"{model_input}\"},\n",
    "]\n",
    "\n",
    "chat_input = tokenizer.apply_chat_template(\n",
    "    chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "tokens = model.generate(**inputs, **generation_args)\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a good french model that can be run properly on CPU. It slow, but it works. Let now use this model to generate 5k questions that can be answered with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text):\n",
    "    \"\"\" Split the text inside the  <|im_start|> assistant <|im_end|> tags and then split the new line text  and return the pair question and response\"\"\"\n",
    "   \n",
    "    # use regex to get the text inside the <|im_start|> assistant <|im_end|> tags\n",
    "    text = text.split(\"<|im_start|> assistant\")[1].split(\"<|im_end|>\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses  = parse_response(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_questions_and_responses(text):\n",
    "    # Define a regex pattern to match questions and their responses\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    pattern = re.compile(\n",
    "        r'(\\d+\\.\\s[^?]+?\\?)\\sRéponse:\\s([^0-9]+?)(?=\\d+\\.|$)', re.DOTALL)\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    # Create a list of tuples with (question, response)\n",
    "    result = [(match[0].strip(), match[1].strip()) for match in matches]\n",
    "    result = [\n",
    "        f\"Question: {question_answer[0]} \\n Réponse: {question_answer[1]}\" for question_answer in result]\n",
    "    return  \"\\n\\n\".join(result)\n",
    "\n",
    "\n",
    "# Extract the questions and responses\n",
    "questions_and_responses = extract_questions_and_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_text = rg.TextField(name=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_question = rg.TextQuestion(\n",
    "    name=\"text\",\n",
    "    title=\"Are this the correct question_answer pairs based on the questions\",\n",
    "    description=\"Please provide feedback on the response\",\n",
    "    required=True,\n",
    "    use_markdown=True\n",
    ")\n",
    "\n",
    "answers = rg.TextField(name=\"answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_setting = rg.Settings(\n",
    "    guidelines=\"Please provide feedback on the response\",\n",
    "    fields=[paragraph_text, answers],\n",
    "    questions = [annotation_question]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset = rg.Dataset(\n",
    "    name=\"annotation_dataset\",\n",
    "    settings=annotation_setting,\n",
    "    client=client,\n",
    "    workspace = \"argilla\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = client.workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = rg.Record(\n",
    "    fields={\"content\": random_article, \"answers\": questions_and_responses},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dataset.records.log([record])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_template(content):\n",
    "  prompt = f\"generate 5 french questions , and for each question one answer in the format (question, answer) based on the following text : {content}\"\n",
    "  chat = [\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "  ]\n",
    "  chat_input = tokenizer.apply_chat_template(\n",
    "      chat, tokenize=False, add_generation_prompt=True)\n",
    "  return chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = Path().cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_path = current_directory.joinpath(\"subset_to_label.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files={'train': subset_path.__str__()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "                      \"prompt_template\": generate_prompt_template(x[\"content\"])}, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = current_directory.joinpath(\"datasets\", \"congo_news_qa\")\n",
    "dataset_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def write_list_to_json_file(list_of_strings, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(list_of_strings, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_and_answers(examples):\n",
    "    \"\"\"\n",
    "    take a batch of example compute the embeddings and save the subset of the embeddings\n",
    "    Add a new columns named embedding to the subsets of example and save the subset locally.\n",
    "    \"\"\"\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.90,\n",
    "        \"top_k\": 40,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "    }\n",
    "\n",
    "    inputs = tokenizer(examples[\"prompt_template\"],\n",
    "                       return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, **generation_args)\n",
    "    questions_and_answers = tokenizer.batch_decode(\n",
    "        outputs, skip_special_tokens=True)\n",
    "    examples[\"question_answers\"] = questions_and_answers\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_shards = 1000 // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shard_index in range(0, numbers_of_shards):\n",
    "    shard = dataset[\"train\"].shard(num_shards=numbers_of_shards, index=shard_index)\n",
    "    shard = shard.map(extract_question_and_answers, batched=True, batch_size=2)\n",
    "    shard.save_to_disk(dataset_path.joinpath(f\"shard_{shard_index}\"))\n",
    "    print(\"done processing shard \", shard_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lesson learned\n",
    "\n",
    "\n",
    "In this section I discoverd how powerfull the croissant LLM model can be on non english task. It was able to generate question on both french and english. \n",
    "\n",
    "I also discover a good annodation tool which is not prodigy, argilla. It has the same capacity as prodigy.\n",
    "\n",
    "T5 is also a good powerful model for question answering, \n",
    "\n",
    "It just the annodation can take a huge amount of time reson why I decide to focus my effort on deploying the Croissant model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
