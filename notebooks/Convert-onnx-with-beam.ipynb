{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.transformers.convert_generation import parse_arguments, GenerationType, convert_generation_model\n",
    "from models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository',\n",
    "                                \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past = onnx_path.joinpath('bio-gpt-model-with-past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Namespace(\n",
    "    model_name_or_path=model_path.__str__(),\n",
    "    output=onnx_model_with_past.parent.joinpath(\n",
    "        \"biogpt-model-with-past-and-beam.onnx\").__str__(),\n",
    "    model_type=\"gpt2\",\n",
    "    num_beams=\"5\",\n",
    "    temperature=\"0.25\",\n",
    "    model_class=\"BioGptModel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_with_past.joinpath(\"model.onnx\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list = []\n",
    "\n",
    "for key, value in vars(arguments).items():\n",
    "    arguments_list.append(f\"--{key}\")\n",
    "    arguments_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--model_name_or_path',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa',\n",
       " '--output',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam.onnx',\n",
       " '--model_type',\n",
       " 'gpt2',\n",
       " '--num_beams',\n",
       " '5',\n",
       " '--temperature',\n",
       " '0.25',\n",
       " '--model_class',\n",
       " 'BioGptModel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(arguments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments:Namespace(model_name_or_path='/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa', model_class='BioGptModel', cache_dir='./cache_models', output='/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_past_fp32.onnx', optimize_onnx=True, use_gpu=False, provider=None, tolerance=0.0005, input_test_file='', precision=<Precision.FLOAT32: 'fp32'>, test_cases=10, test_runs=1, verbose=False, use_external_data_format=False, overwrite=True, use_int64_inputs=False, stage=0, auto_mixed_precision=False, keep_io_types=False, io_block_list=[], op_block_list=[], node_block_list=[], force_fp16_initializers=False)\n",
      "PyTorch Version:2.2.0\n",
      "Transformers Version:4.38.2\n",
      "Onnxruntime Version:1.15.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usign the model class <class 'gpt2_helper.MyBioGptModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting ONNX model to /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_BioGptModel_past.onnx\n",
      "Shapes: input_ids=torch.Size([1, 1]) past=torch.Size([2, 1, 16, 1, 64]) output=torch.Size([1, 1, 42393]) present=torch.Size([2, 1, 16, 2, 64])\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:495: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif attention_mask.shape[1] != past_key_values_length + input_shape[1]:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:184: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:223: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Optimizing model to /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_BioGptModel_past_fp32.onnx\n",
      "Removed 3 Cast nodes with output type same as input\n",
      "Fused LayerNormalization: 49\n",
      "Fused Gelu: 24\n",
      "Fused SkipLayerNormalization: 48\n",
      "Remove reshape node /biogpt/Reshape since its input shape is same as output: [4]\n",
      "Removed 1 nodes\n",
      "postprocess: remove Reshape count:0\n",
      "Fused BiasGelu: 24\n",
      "Fused SkipLayerNormalization(add bias): 47\n",
      "opset version: 11\n",
      "Sort graphs in topological order\n",
      "Model saved to /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_BioGptModel_past_fp32.onnx\n",
      "Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'MultiHeadAttention': 0, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 24, 'GemmFastGelu': 0, 'LayerNormalization': 1, 'SkipLayerNormalization': 48, 'QOrderedAttention': 0, 'QOrderedGelu': 0, 'QOrderedLayerNormalization': 0, 'QOrderedMatMul': 0}\n",
      "Output path: /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_past_fp32.onnx\n",
      "Done. Output model: /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_past_fp32.onnx\n",
      "Creating an initial run GPT2 decoder from /Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa_past_fp32.onnx. \n",
      "Tried and failed to generate the init decoder GPT2 model. Performance may be sub-optimal for the initial decoding run\n",
      "You are using a model of type biogpt to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Verifying GPT-2 graph inputs: name and data type are good.\n",
      "Verifying GPT-2 graph outputs: name and data type are good.\n",
      "389 initializers from the decoder are moved to the main graph\n",
      "model save to /Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam.onnx\n"
     ]
    }
   ],
   "source": [
    "convert_generation_model(args=args, generation_type=GenerationType.BEAMSEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model for the inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_beam = onnx_model_with_past.parent.joinpath(\n",
    "        \"biogpt-model-with-past-and-beam.onnx\").__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:25:16.375628 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-05 11:25:16.375654 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/Constant_8_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-05 11:25:16.404403 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/Constant_output_0'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "inference_session = InferenceSession(model_with_beam, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, it worked.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_Text = [\"what is the cause of covid-19?\"]\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name_or_path: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, cache_dir=cache_dir)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_path.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_path, model_path)\n",
    "encodings_dict = tokenizer.batch_encode_plus(EXAMPLE_Text, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(encodings_dict[\"input_ids\"], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  1994,    21,     6,   533,     5,  1181, 17270,     9,   656,\n",
       "           927]], dtype=torch.int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64)),\n",
       " tensor([], size=(2, 1, 16, 0, 64))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"input_ids\": np.ascontiguousarray(input_ids.cpu().numpy(), dtype=np.int32),\n",
    "    \"max_length\": np.array([512], dtype=np.int32),\n",
    "    \"min_length\": np.array([128], dtype=np.int32),\n",
    "    \"num_beams\": np.array([5], dtype=np.int32),\n",
    "    \"num_return_sequences\": np.array([1], dtype=np.int32),\n",
    "    \"length_penalty\": np.array([1.0], dtype=np.float32),\n",
    "    \"repetition_penalty\": np.array([1.4], dtype=np.float32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = inference_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indices = outputs[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_path.__str__(), model_path.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the cause of covid-19? context: The disease is known to have an impact on morbi-mortality. We present a clinical case study of a 60-year-old female, with a history of hypertension, who was admitted to the hospital due to cough, sore throat, fever and weight loss. The patient had a medical history of hypertension, in which she had been treated for several years, with poor control of blood pressure. The physical examination revealed a cervical adenopathy, without any other abnormalities. The chest x-ray showed a mediastinal mass, which was confirmed by a biopsy of the cervical lymph nodes. The patient was diagnosed with primary mediastinal large B-cell lymphoma, stage IV. The patient was treated with chemotherapy, but the disease did not improve until she died 7 months later. <unk><unk><unk><unk><unk><unk><unk><unk><unk>the answer to the question given the context is yes.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_indices, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code seems to be working, we can continue with the deployment and write the blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
