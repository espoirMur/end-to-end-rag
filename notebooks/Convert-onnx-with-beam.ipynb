{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.convert_generation import (\n",
    "    GenerationType,\n",
    "    convert_generation_model,\n",
    "    parse_arguments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository',\n",
    "                                \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past = onnx_path.joinpath('bio-gpt-model-with-past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Namespace(\n",
    "    model_name_or_path=model_path.__str__(),\n",
    "    output=onnx_model_with_past.parent.joinpath(\n",
    "        \"biogpt-model-with-past-and-beam.onnx\").__str__(),\n",
    "    model_type=\"gpt2\",\n",
    "    num_beams=\"5\",\n",
    "    temperature=\"0.25\",\n",
    "    model_class=\"BioGptModel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past.joinpath(\"model.onnx\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list = []\n",
    "\n",
    "for key, value in vars(arguments).items():\n",
    "    arguments_list.append(f\"--{key}\")\n",
    "    arguments_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(arguments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_generation_model(args=args, generation_type=GenerationType.BEAMSEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model for the inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_beam = onnx_model_with_past.parent.joinpath(\n",
    "        \"biogpt-model-with-past-and-beam.onnx\").__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_session = InferenceSession(model_with_beam, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, it worked.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_Text = [\"what is the cause of covid-19?\"]\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name_or_path: str, cache_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, cache_dir=cache_dir)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_path.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_path, model_path)\n",
    "encodings_dict = tokenizer.batch_encode_plus(EXAMPLE_Text, padding=True)\n",
    "\n",
    "input_ids = torch.tensor(encodings_dict[\"input_ids\"], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"input_ids\": np.ascontiguousarray(input_ids.cpu().numpy(), dtype=np.int32),\n",
    "    \"max_length\": np.array([512], dtype=np.int32),\n",
    "    \"min_length\": np.array([128], dtype=np.int32),\n",
    "    \"num_beams\": np.array([5], dtype=np.int32),\n",
    "    \"num_return_sequences\": np.array([1], dtype=np.int32),\n",
    "    \"length_penalty\": np.array([1.0], dtype=np.float32),\n",
    "    \"repetition_penalty\": np.array([1.4], dtype=np.float32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = inference_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indices = outputs[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_path.__str__(), model_path.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output_indices, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code seems to be working, we can continue with the deployment and write the blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come back and test the triton inputs and outputs.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
