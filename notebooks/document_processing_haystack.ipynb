{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to create embedding for the congonews databaset.\n",
    "\n",
    "The dataset is has around 87k documents I will create embedding for those document and ingest them in a postgres database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "database_user = getenv('POSTGRES_USER')\n",
    "database_password = getenv('POSTGRES_PASSWORD')\n",
    "database_host = getenv('POSTGRES_HOST')\n",
    "database_port = getenv('POSTGRES_PORT')\n",
    "database_name = getenv('POSTGRES_DB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri = f'postgresql://{database_user}:{quote(database_password)}@{database_host}:{database_port}/{database_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "from psycopg2 import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = connect(\n",
    "    user=database_user,\n",
    "    password=database_password,\n",
    "    host=database_host,\n",
    "    port=database_port,\n",
    "    database=database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with database_connection.cursor() as cursor:\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_vector(database_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_creation_string = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS article_embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    article_id INTEGER,\n",
    "    chunk TEXT,\n",
    "    chunk_vector VECTOR(768),\n",
    "    CONSTRAINT fk_article_id FOREIGN KEY (article_id) REFERENCES article(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "insert_statement_string = \"\"\"\n",
    "INSERT INTO article_embeddings (article_id, chunk, chunk_vector)\n",
    "values (%(article_id)s, %(chunk)s, %(chunk_vector)s)\n",
    "\"\"\"\n",
    "\n",
    "update_statement_string = \"\"\"\n",
    "ON CONFLICT (id) DO UPDATE SET\n",
    "article_id = EXCLUDED.article_id,\n",
    "chunk = EXCLUDED.chunk,\n",
    "chunk_vector = EXCLUDED.chunk_vector,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(database_connection, query, params=None) -> Optional[List[Any]]:\n",
    "    with database_connection.cursor() as cursor:\n",
    "        cursor.execute(query, params)\n",
    "        try:\n",
    "            return cursor.fetchall()\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with database_connection.cursor() as cursor:\n",
    "    cursor.execute(table_creation_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the trickiest part, we need to load the article, split the article into chunk, compute the embedding for each chunk and then save the embedding as chunk in the vector!\n",
    "\n",
    "I will start simple with querying the database and load around 60 document and save those documents. Then we will scale the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "\n",
    "In this section we will load news data table from the database to the huggingface dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Value\n",
    "\n",
    "\"\"\"\n",
    "    id             | integer                     |           | not null | nextval('article_id_seq'::regclass)\n",
    " title          | character varying(250)      |           | not null |\n",
    " content        | text                        |           | not null |\n",
    " summary        | text                        |           |          |\n",
    " posted_at      | timestamp without time zone |           |          |\n",
    " website_origin | character varying(250)      |           |          |\n",
    " url            | character varying(250)      |           |          |\n",
    " author         | character varying(250)      |           |          |\n",
    " saved_at       | timestamp with time zone\n",
    "\"\"\"\n",
    "\n",
    "features = Features({\n",
    "    'id': Value('int32'),\n",
    "    'title': Value('string'),\n",
    "    'content': Value('string'),\n",
    "    'summary': Value('string'),\n",
    "    'posted_at': Value('string'),\n",
    "    'website_origin': Value('string'),\n",
    "    'url': Value('string'),\n",
    "    'author': Value('string'),\n",
    "    'saved_at': Value('string'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congo_news_dataset = Dataset.from_sql(\n",
    "    'article', postgres_uri, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create random indexes from the dataset.\n",
    "\n",
    "import random\n",
    "\n",
    "random_indexes = random.sample(range(len(congo_news_dataset)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_to_label = congo_news_dataset.select(random_indexes).select_columns([\"id\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_to_label.to_parquet(\"subset_to_label.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystack_documents = [\n",
    "    Document(content=example['content'], id=example[\"id\"], meta={}) for example in congo_news_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "\n",
    "document_cleaner = DocumentCleaner(remove_substrings=[\n",
    "                                   r\"This post has already been read \\d+ times!\"],\n",
    "                                   remove_regex=\"\",\n",
    "                                   keep_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample one number from the range 1 to 10\n",
    "random_document_id = random.randint(1, len(haystack_documents))\n",
    "print(random_document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents = document_cleaner.run(haystack_documents[random_document_id:random_document_id+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents.get(\"documents\")[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.components.document_splitter import RecursiveCharacterTextSplitterComponent\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitterComponent(\n",
    "    chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = recursive_text_splitter.run(haystack_documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive character splitter offer a better retrieveal accuracy for the reason specified here: https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us build the document Store\n",
    "\n",
    "I will come back to this, create the document store.vb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.document_store import MyPgVectorDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils.auth import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = MyPgVectorDocumentStore(\n",
    "    embedding_dimension=768,\n",
    "    vector_function=\"cosine_similarity\",\n",
    "    recreate_table=False,\n",
    "    table_name=\"article_embeddings\",\n",
    "    connection_string=Secret.from_env_var(\"PG_CONN_STR\"),\n",
    "    sql_insert_string=insert_statement_string,\n",
    "    sql_update_string=update_statement_string,\n",
    "    language=\"french\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_component = SentenceTransformersDocumentEmbedder(\n",
    "    model=model_id,\n",
    "    normalize_embeddings=True,\n",
    "\n",
    ")\n",
    "embedder_component.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_with_embeddings = embedder_component.run(split_documents.get(\"documents\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if the model is working, but I will come back here to check if the model was working.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Documents to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_writer = DocumentWriter(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_writer.run(document_with_embeddings.get(\"documents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_keyword_query_string = \"\"\"\n",
    "select article_id, chunk, ts_rank_cd(to_tsvector(%(language)s, chunk), query) as score\n",
    "from article_embeddings, plainto_tsquery(%(language)s, %(query)s) query\n",
    "where to_tsvector(%(language)s, chunk) @@ query \n",
    "order by score desc \n",
    "limit %(limit)s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_keyword_query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = execute_query(database_connection, select_by_keyword_query_string, {\"language\": \"french\", \"query\": \"francophonie\", \"limit\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have tested that we can write the document in our datastore, let us now write all the document to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.add_component(\"text_cleaner\", document_cleaner)\n",
    "index_pipeline.add_component(\"text_splitter\", recursive_text_splitter)\n",
    "index_pipeline.add_component(\"embedder\", embedder_component)\n",
    "index_pipeline.add_component(\"writer\", document_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.connect(\"text_cleaner\", \"text_splitter\")\n",
    "index_pipeline.connect(\"text_splitter\", \"embedder\")\n",
    "index_pipeline.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.run( {\"documents\": haystack_documents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ingestion pipeline is completed.\n",
    "\n",
    "A lot can be done to improve it. I had 86k documents after splitting I ended up with 398k document. Which mean we had on average 4 split per documents.\n",
    "I will try do the retrieval with that, in the future I will improve the data ingestion to handle a huge volume of data.\n",
    "\n",
    "- I can compute the embeddings, then use an async function to bulk insert each batch to the database.\n",
    "- I can compute the embedding on a jupyter notebbok using colab GPU and then save the document as parquet files, then ingest the parquet file to dthe database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
