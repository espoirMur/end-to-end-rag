{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to create embedding for the congonews databaset.\n",
    "\n",
    "The dataset is has around 87k documents I will create embedding for those document and ingest them in a postgres database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "database_user = getenv('POSTGRES_USER')\n",
    "database_password = getenv('POSTGRES_PASSWORD')\n",
    "database_host = getenv('POSTGRES_HOST')\n",
    "database_port = getenv('POSTGRES_PORT')\n",
    "database_name = getenv('POSTGRES_DB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri = f'postgresql://{database_user}:{quote(database_password)}@{database_host}:{database_port}/{database_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import connect\n",
    "from pgvector.psycopg2 import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = connect(\n",
    "    user=database_user,\n",
    "    password=database_password,\n",
    "    host=database_host,\n",
    "    port=database_port,\n",
    "    database=database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with database_connection.cursor() as cursor:\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_vector(database_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_creation_string = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS article_embeddings (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    article_id INTEGER,\n",
    "    chunk TEXT,\n",
    "    chunk_vector VECTOR(768),\n",
    "    CONSTRAINT fk_article_id FOREIGN KEY (article_id) REFERENCES article(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "insert_statement_string = \"\"\"\n",
    "INSERT INTO article_embeddings (article_id, chunk, chunk_vector)\n",
    "values (%(article_id)s, %(chunk)s, %(chunk_vector)s)\n",
    "\"\"\"\n",
    "\n",
    "update_statement_string = \"\"\"\n",
    "ON CONFLICT (id) DO UPDATE SET\n",
    "article_id = EXCLUDED.article_id,\n",
    "chunk = EXCLUDED.chunk,\n",
    "chunk_vector = EXCLUDED.chunk_vector,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(database_connection, query, params=None) -> Optional[List[Any]]:\n",
    "    with database_connection.cursor() as cursor:\n",
    "        cursor.execute(query, params)\n",
    "        try:\n",
    "            return cursor.fetchall()\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with database_connection.cursor() as cursor:\n",
    "    cursor.execute(table_creation_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the trickiest part, we need to load the article, split the article into chunk, compute the embedding for each chunk and then save the embedding as chunk in the vector!\n",
    "\n",
    "I will start simple with querying the database and load around 60 document and save those documents. Then we will scale the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "\n",
    "In this section we will load news data table from the database to the huggingface dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value, Features\n",
    "\n",
    "\"\"\"\n",
    "    id             | integer                     |           | not null | nextval('article_id_seq'::regclass)\n",
    " title          | character varying(250)      |           | not null |\n",
    " content        | text                        |           | not null |\n",
    " summary        | text                        |           |          |\n",
    " posted_at      | timestamp without time zone |           |          |\n",
    " website_origin | character varying(250)      |           |          |\n",
    " url            | character varying(250)      |           |          |\n",
    " author         | character varying(250)      |           |          |\n",
    " saved_at       | timestamp with time zone\n",
    "\"\"\"\n",
    "\n",
    "features = Features({\n",
    "    'id': Value('int32'),\n",
    "    'title': Value('string'),\n",
    "    'content': Value('string'),\n",
    "    'summary': Value('string'),\n",
    "    'posted_at': Value('string'),\n",
    "    'website_origin': Value('string'),\n",
    "    'url': Value('string'),\n",
    "    'author': Value('string'),\n",
    "    'saved_at': Value('string'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congo_news_dataset = Dataset.from_sql(\n",
    "    'article', postgres_uri, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystack_documents = [\n",
    "    Document(content=example['content'], id=example[\"id\"], meta={}) for example in congo_news_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "\n",
    "\n",
    "document_cleaner = DocumentCleaner(remove_substrings=[\n",
    "                                   r\"This post has already been read \\d+ times!\"], \n",
    "                                   remove_regex=\"\",\n",
    "                                   keep_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample one number from the range 1 to 10\n",
    "random_document_id = random.randint(1, len(haystack_documents))\n",
    "print(random_document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents = document_cleaner.run(haystack_documents[random_document_id:random_document_id+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents.get(\"documents\")[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.document_splitter import RecursiveCharacterTextSplitterComponent\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitterComponent(\n",
    "    chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = recursive_text_splitter.run(haystack_documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive character splitter offer a better retrieveal accuracy for the reason specified here: https://www.reddit.com/r/LangChain/comments/1bjxvov/what_is_the_advantage_of_overlapping_in_chunking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us build the document Store\n",
    "\n",
    "I will come back to this, create the document store.vb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.document_store import MyPgVectorDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PG_CONN_STR\"] = postgres_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils.auth import Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = MyPgVectorDocumentStore(\n",
    "    embedding_dimension=768,\n",
    "    vector_function=\"cosine_similarity\",\n",
    "    recreate_table=False,\n",
    "    table_name=\"article_embeddings\",\n",
    "    connection_string=Secret.from_env_var(\"PG_CONN_STR\"),\n",
    "    sql_insert_string=insert_statement_string,\n",
    "    sql_update_string=update_statement_string,\n",
    "    language=\"french\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_component = SentenceTransformersDocumentEmbedder(\n",
    "    model=model_id,\n",
    "    normalize_embeddings=True,\n",
    "    \n",
    ")\n",
    "embedder_component.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_with_embeddings = embedder_component.run(split_documents.get(\"documents\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure if the model is working, but I will come back here to check if the model was working.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Documents to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_writer = DocumentWriter(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_writer.run(document_with_embeddings.get(\"documents\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg.sql import Literal as SQLLiteral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_keyword_query_string = \"\"\"\n",
    "select article_id, chunk, ts_rank_cd(to_tsvector(%(language)s, chunk), query) as score\n",
    "from article_embeddings, plainto_tsquery(%(language)s, %(query)s) query\n",
    "where to_tsvector(%(language)s, chunk) @@ query \n",
    "order by score desc \n",
    "limit %(limit)s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_keyword_query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = execute_query(database_connection, select_by_keyword_query_string, {\"language\": \"french\", \"query\": \"francophonie\", \"limit\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have tested that we can write the document in our datastore, let us now write all the document to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.add_component(\"text_cleaner\", document_cleaner)\n",
    "index_pipeline.add_component(\"text_splitter\", recursive_text_splitter)\n",
    "index_pipeline.add_component(\"embedder\", embedder_component)\n",
    "index_pipeline.add_component(\"writer\", document_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.connect(\"text_cleaner\", \"text_splitter\")\n",
    "index_pipeline.connect(\"text_splitter\", \"embedder\")\n",
    "index_pipeline.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_pipeline.run( {\"documents\": haystack_documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
