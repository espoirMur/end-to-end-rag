{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will use the the croissant LLM to answer my questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer, set_seed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"croissantllm/CroissantLLMChat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.retrieval import HybridRetriever\n",
    "model_id = \"camembert-base\"\n",
    "spacy_model = 'fr_core_news_md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HybridRetriever(model_id=model_id, spacy_model=spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Quand l’Ordonnance présidentielle a-t-elle été lue sur le plateau de la Radiotélévision nationale congolaise (RTNC)?\",\n",
    "             \"Qui a été nommé pour remplacer Emmanuel Ramazani Shadary au poste de vice-Premier ministre et ministre de l’Intérieur et sécurité?\",\n",
    "             \"Où et quand Henri Mova Sakanyi est-il né?\",\n",
    "             \"Quelle est la carrière politique de Henri Mova Sakanyi en République démocratique du Congo?\",\n",
    "             \"Quel est le poste actuel de Henri Mova Sakanyi au sein du Parti du peuple pour la Reconstruction et la Démocratie (PPRD)?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_input(query:str, documents:list) -> str:\n",
    "    prompt_template  = \"\"\"\n",
    "    Given the following information Context:\n",
    "        {% for document in documents %}\n",
    "            {{ document }}\n",
    "        {% endfor %}\n",
    "    answer the question : {{question}} in French.\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    template = Template(prompt_template)\n",
    "    prompt = template.render(documents=documents, question=query)\n",
    "    \n",
    "    chat_input = [\n",
    "      {\"role\": \"system\", \"content\": \"You answer questions about news in Democratic Republic of the Congo in French.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    return chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text):\n",
    "    \"\"\" Split the text inside the  <|im_start|> assistant <|im_end|> tags and then split the new line text  and return the pair question and response\"\"\"\n",
    "\n",
    "    # use regex to get the text inside the <|im_start|> assistant <|im_end|> tags\n",
    "    text = text.split(\"<|im_start|> assistant\")[1].split(\"<|im_end|>\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(chat_input:str) -> str:\n",
    "    \"\"\"this function generates an answer to a question given a chat input\n",
    "\n",
    "    Args:\n",
    "        chat_input (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.90,\n",
    "        \"top_k\": 40,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "    }\n",
    "\n",
    "\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = model.generate(**inputs, **generation_args)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    generated_token = tokens[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(generated_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions[:1]:\n",
    "    documents = retriever.run(question)\n",
    "    chat_input = generate_chat_input(question, documents)\n",
    "    chat_input = tokenizer.apply_chat_template(\n",
    "        chat_input, tokenize=False, add_generation_prompt=True)\n",
    "    print(chat_input)\n",
    "    answer = generate_answer(chat_input)\n",
    "    # answer = parse_response(answer)\n",
    "    print(\"the answer to the question {} is: __ \\n {}\".format(question, answer))\n",
    "\n",
    "    print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still halucinate, it may need some fine-tuning but let move to it's deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have manged to run the model, let me think about the deployment, in this I will use either llam c++ or trition inference server.\n",
    "\n",
    " https://github.com/ggerganov/llama.cpp?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_id = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {v: k for k, v in tokens_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(id_to_token) == len(tokens_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for prediction\n",
    "\n",
    "Bellow is the code that call the llama server Api to get the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def encode_header(message):\n",
    "    tokens = f\"<|start_header_id|>{message['role']}<|end_header_id|>\\n\\n\"\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def encode_message(message):\n",
    "    tokens = encode_header(message)\n",
    "    tokens += message[\"content\"].strip() + \"<|eot_id|>\"\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def encode_dialog_prompt(dialog):\n",
    "    tokens = \"<|begin_of_text|>\"\n",
    "    for message in dialog:\n",
    "        tokens += encode_message(message)\n",
    "\n",
    "    tokens += encode_header({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    return tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the URL\n",
    "url = 'http://localhost:8001/completion'\n",
    "\n",
    "# Define the headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_prompts = []\n",
    "\n",
    "dialog = generate_chat_input(questions[1], documents)\n",
    "batched_prompts.append(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_prompts[0][1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "data = {\n",
    "    \"prompt\": batched_prompts[0][1][\"content\"],\n",
    "    \"n_predict\": 512,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_k\": 40,\n",
    "    \"top_p\": 0.90,\n",
    "    \"stopped_eos\": True,\n",
    "    \"repeat_penalty\": 1.05,\n",
    "    \"stop\": [],\n",
    "}\n",
    "\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(url, headers=headers, data=json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
