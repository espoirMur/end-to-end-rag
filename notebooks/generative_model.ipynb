{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook where I will implement the generator model.\n",
    "\n",
    "In this notebook I will try different model to generate answers:\n",
    "\n",
    "- bofenghuang/vigogne-2-13b-instruct : This to be a llama version but it is huge. \n",
    "\n",
    "- ClassCat/gpt2-base-french: the most used gpt model.\n",
    "\n",
    "- moussaKam/barthez : which use the BART model suitable for generation tasks.\n",
    "\n",
    "- JDBN/t5-base-fr-qg-fquad: which use T5 model.\n",
    "\n",
    "Additional I I find time to annodate I will try BERT model to extract model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Quand l’Ordonnance présidentielle a-t-elle été lue sur le plateau de la Radiotélévision nationale congolaise (RTNC)?\",\n",
    "             \"Qui a été nommé pour remplacer Emmanuel Ramazani Shadary au poste de vice-Premier ministre et ministre de l’Intérieur et sécurité?\",\n",
    "             \"Où et quand Henri Mova Sakanyi est-il né?\",\n",
    "             \"Quelle est la carrière politique de Henri Mova Sakanyi en République démocratique du Congo?\",\n",
    "             \"Quel est le poste actuel de Henri Mova Sakanyi au sein du Parti du peuple pour la Reconstruction et la Démocratie (PPRD)?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = 'fr_core_news_md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.retriever.retrieval import HybridRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HybridRetriever(model_id=model_id, spacy_model=spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Answer:\")\n",
    "    for i, document in enumerate(documents):\n",
    "        print(f\"Document {i}: {document}\")\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = retriever.run(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = \" \".join([document for document in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model_id = \"JDBN/t5-base-fr-qg-fquad\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(generative_model_id)\n",
    "tokenizer = T5Tokenizer.from_pretrained(generative_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"question: {question} context: {contexts}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=512,\n",
    "                          truncation=True)\n",
    "output = model.generate(input_ids=encoded_input.input_ids,\n",
    "                        attention_mask=encoded_input.attention_mask,\n",
    "                        do_sample=True,)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    contexts = \" \".join([document for document in documents])\n",
    "    input_ = f\"question: {question} context: {contexts}\"\n",
    "    encoded_input = tokenizer([input_],\n",
    "                              return_tensors='pt')\n",
    "    output = model.generate(input_ids=encoded_input.input_ids,\n",
    "                            attention_mask=encoded_input.attention_mask,\n",
    "                            do_sample=True,\n",
    "                            top_k=5,\n",
    "                            temperature=0.6)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {output}\")\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "bert_model_id = \"AgentPublic/camembert-base-squadFR-fquad-piaf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertForQuestionAnswering, CamembertTokenizer\n",
    "\n",
    "bert_tokenizer = CamembertTokenizer.from_pretrained(bert_model_id)\n",
    "bert_model = CamembertForQuestionAnswering.from_pretrained(bert_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    contexts = \" \".join([document for document in documents])\n",
    "    input = f\"question: {question} context: {contexts}\"\n",
    "    encoded_input = bert_tokenizer(input, return_tensors='pt', max_length=512, truncation=True)\n",
    "    output = bert_model(**encoded_input)\n",
    "    start_token_index = torch.argmax(output.start_logits)\n",
    "    end_token_index = torch.argmax(output.end_logits)\n",
    "    print(f\"Question: {question}\")\n",
    "    answers_token = encoded_input['input_ids'][0][start_token_index:end_token_index+1]\n",
    "    answer = bert_tokenizer.decode(answers_token)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not got at generating response, I will come back and try BART and GPT2 and Alpaca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model_id = \"ClassCat/gpt2-base-french\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"ClassCat/gpt2-base-french\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"ClassCat/gpt2-base-french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    contexts = \" \".join([document for document in documents])\n",
    "    input_ = f\"question: {question} context: {contexts}\"\n",
    "    encoded_input = gpt2_tokenizer([input_],\n",
    "                              return_tensors='pt')\n",
    "    output = gpt2_model.generate(input_ids=encoded_input.input_ids,\n",
    "                            attention_mask=encoded_input.attention_mask,\n",
    "                            do_sample=True,\n",
    "                            top_k=5,\n",
    "                            temperature=0.6,\n",
    "                            max_new_tokens=30)\n",
    "    output = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = output.replace(input_, '')\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barthez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "bartez_tokenizer = AutoTokenizer.from_pretrained(\"moussaKam/barthez\")\n",
    "bartez_model = MBartForConditionalGeneration.from_pretrained(\n",
    "    \"moussaKam/barthez\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    contexts = \" \".join([document for document in documents])\n",
    "    input_ = f\"question: {question} context: {contexts}\"\n",
    "    encoded_input = bartez_tokenizer([input_],\n",
    "                              return_tensors='pt')\n",
    "    output = bartez_model.generate(input_ids=encoded_input.input_ids,\n",
    "                            attention_mask=encoded_input.attention_mask,\n",
    "                            do_sample=True,\n",
    "                            top_k=5,\n",
    "                            temperature=0.6,)\n",
    "    output = bartez_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = output.replace(input_, '')\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"----\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all those model, the T5 model seems to give some promising results.\n",
    "\n",
    "We will use it to generate questions and then use it to answer the question it generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.deepset.ai/blog/generate-questions-automatically-for-faster-annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
