{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with the LLama Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will call the croissant LLM to generate the response from our questions. We will compare the first approach that call the model directly from the transformer library and then the second approach that hit the llam-cpp api.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer, set_seed\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"croissantllm/CroissantLLMChat-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16, device_map=\"auto\", offload_folder=\"offload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.components.retriever import HybridRetriever\n",
    "model_id = \"camembert-base\"\n",
    "spacy_model = 'fr_core_news_md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HybridRetriever(model_id=model_id, spacy_model=spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Quand l’Ordonnance présidentielle a-t-elle été lue sur le plateau de la Radiotélévision nationale congolaise (RTNC)?\",\n",
    "             \"Qui a été nommé pour remplacer Emmanuel Ramazani Shadary au poste de vice-Premier ministre et ministre de l’Intérieur et sécurité?\",\n",
    "             \"Où et quand Henri Mova Sakanyi est-il né?\",\n",
    "             \"Quelle est la carrière politique de Henri Mova Sakanyi en République démocratique du Congo?\",\n",
    "             \"Quel est le poste actuel de Henri Mova Sakanyi au sein du Parti du peuple pour la Reconstruction et la Démocratie (PPRD)?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using The Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_input(query:str, documents:list) -> str:\n",
    "    prompt_template  = \"\"\"\n",
    "        Context:\n",
    "        {% for document in documents %}\n",
    "            {{ document }}\n",
    "        {% endfor %}\n",
    "\n",
    "        Question: {{question}}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    template = Template(prompt_template)\n",
    "    prompt = template.render(documents=documents, question=query)\n",
    "    \n",
    "    chat_input = [\n",
    "        {\"role\": \"system\", \"content\": \"Given the Context:, answer the question in french.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    return chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text):\n",
    "    \"\"\" Split the text inside the  <|im_start|> assistant <|im_end|> tags and then split the new line text  and return the pair question and response\"\"\"\n",
    "\n",
    "    # use regex to get the text inside the <|im_start|> assistant <|im_end|> tags\n",
    "    text = text.split(\"<|im_start|> assistant\")[1].split(\"<|im_end|>\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(chat_input:str) -> str:\n",
    "    \"\"\"this function generates an answer to a question given a chat input\n",
    "\n",
    "    Args:\n",
    "        chat_input (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "   \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.90,\n",
    "        \"top_k\": 40,\n",
    "        \"repetition_penalty\": 1.05,\n",
    "        \"eos_token_id\": [tokenizer.eos_token_id, 32000],\n",
    "    }\n",
    "\n",
    "\n",
    "    inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = model.generate(**inputs, **generation_args)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    generated_token = tokens[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(generated_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions[:1]:\n",
    "    documents = retriever.run(question)\n",
    "    chat_input = generate_chat_input(question, documents)\n",
    "    chat_tokens = tokenizer.apply_chat_template(\n",
    "        chat_input, tokenize=False, add_generation_prompt=True)\n",
    "    answer = generate_answer(chat_tokens)\n",
    "    answer = parse_response(answer)\n",
    "    print(\"the answer to the question {} is: __ \\n {}\".format(question, answer))\n",
    "\n",
    "    print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hit the LLama API\n",
    "\n",
    "Bellow is the code that call the llama server Api to get the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the URL\n",
    "API_URL = 'http://localhost:8001/completion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = retriever.run(questions[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = generate_chat_input(questions[1], documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_tokens = tokenizer.apply_chat_template(\n",
    "    chat_input, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_from_llama_api(prompt:str) ->str:\n",
    "    \"\"\" \n",
    "    This function sends a post request to the llama api and returns the response.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": 128,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 0.90,\n",
    "        \"stopped_eos\": True,\n",
    "        \"repeat_penalty\": 1.05,\n",
    "        \"stop\": [\"assistant\", tokenizer.eos_token],\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "\n",
    "    json_data = json.dumps(data)\n",
    "\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, data=json_data)\n",
    "        return response.json()[\"content\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    documents = retriever.run(question)\n",
    "    chat_input = generate_chat_input(question, documents)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        chat_input, tokenize=False, add_generation_prompt=True)\n",
    "    answer = generate_response_from_llama_api(prompt)\n",
    "    print(\"the answer to the question {} is: __ \\n {}\".format(question, answer))\n",
    "\n",
    "    print(50 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Class to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.components.generator import LLamaCppGeneratorComponent\n",
    "from src.rag.components.retriever import HybridRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = HybridRetriever(model_id=model_id, spacy_model=spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = questions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = hybrid_retriever.run(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the URL\n",
    "API_URL = 'http://localhost:8001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_component = LLamaCppGeneratorComponent(api_url=API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert generator_component._ping_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generator_component.run(sample_question, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the answer to the question {} is: __ \\n {}\".format(sample_question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
