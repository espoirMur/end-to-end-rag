{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onnx Model conversion and Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note load the stuff about ONNX runtime from the machine translation tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path.parent.parent.exists(\n",
    "), f\"Model not found at {model_path.parent.parent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\n",
    "\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_path,  local_files_only=True)\n",
    "model = BioGptForCausalLM.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input):\n",
    "    return tokenizer([input],\n",
    "                     return_tensors='pt',\n",
    "                     max_length=1024,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.onnx import FeaturesManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"seq2seq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"'question:what is the cause of covid ? context: the cause of covid is a virus'\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "generate_tokens = model.generate(**encoded_input,\n",
    "                                 num_beams=5,\n",
    "                                 do_sample=True,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository', \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mgenerate_tokens\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(generate_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'past' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpast\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'past' is not defined"
     ]
    }
   ],
   "source": [
    "past[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export with Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be impossible, let us use the model optimum libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx.model_configs import GPT2OnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "class CustomBioGPTConfig(GPT2OnnxConfig):\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, \n",
    "                 task: str = \"text-generation-with-past\", \n",
    "                 int_dtype: str = \"int64\", \n",
    "                 float_dtype: str = \"fp32\", \n",
    "                 use_past: bool = True, \n",
    "                 use_past_in_inputs: bool = True, \n",
    "                 preprocessors: List[Any] | None = None, legacy: bool = False):\n",
    "        super().__init__(config, task, int_dtype, float_dtype, use_past, use_past_in_inputs, preprocessors, legacy)\n",
    "        self._config.n_layer = config.num_hidden_layers\n",
    "        self._config.n_head = config.num_attention_heads\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_inputs = {\"input_ids\": {0: \"batch_size\", 1: \"sequence\"}, \n",
    "                        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}}\n",
    "        \n",
    "        self.add_past_key_values(common_inputs, direction=\"inputs\")\n",
    "        return common_inputs\n",
    "    \n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_outputs = OrderedDict({\"logits\": {0: \"batch_size\", 1: \"sequence\"}})\n",
    "        self.add_past_key_values(common_outputs, direction=\"outputs\")\n",
    "    \n",
    "        return common_outputs\n",
    "    \n",
    "    def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\n",
    "        \"\"\"\n",
    "        Fills `input_or_outputs` mapping with past_key_values dynamic axes considering the direction.\n",
    "\n",
    "        Args:\n",
    "            inputs_or_outputs (`Dict[str, Dict[int, str]]`):\n",
    "                The mapping to fill.\n",
    "            direction (`str`):\n",
    "                either \"inputs\" or \"outputs\", it specifies whether `input_or_outputs` is the input mapping or the\n",
    "                output mapping, this is important for axes naming.\n",
    "        \"\"\"\n",
    "        if direction not in [\"inputs\", \"outputs\"]:\n",
    "            raise ValueError(\n",
    "                f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n",
    "\n",
    "        if direction == \"inputs\":\n",
    "            decoder_sequence_name = \"past_seq_len\"\n",
    "            name = \"past_key_values\"\n",
    "        else:\n",
    "            decoder_sequence_name = \"total_seq_len\"\n",
    "            name = \"present\"\n",
    "\n",
    "        for i in range(self._normalized_config.num_layers):\n",
    "            inputs_or_outputs[f\"{name}.{i}.key\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}\n",
    "            inputs_or_outputs[f\"{name}.{i}.value\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = CustomBioGPTConfig(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to combeback here later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 2.2.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:495: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif attention_mask.shape[1] != past_key_values_length + input_shape[1]:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:184: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:223: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Post-processing the exported models...\n",
      "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
      "Validating ONNX model /Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (present.9.key, present.19.value, present.23.value, present.2.value, present.22.value, present.14.key, present.23.key, present.13.key, present.15.value, present.3.key, present.10.key, present.1.value, present.0.value, present.11.key, present.12.value, present.7.value, present.11.value, present.6.value, present.17.value, logits, present.19.key, present.21.key, present.2.key, present.22.key, present.5.value, present.6.key, present.14.value, present.15.key, present.18.value, present.21.value, present.5.key, present.3.value, present.17.key, present.8.value, present.7.key, present.4.key, present.4.value, present.10.value, present.20.value, present.8.key, present.1.key, present.0.key, present.12.key, present.18.key, present.20.key, present.16.key, present.9.value, present.16.value, present.13.value)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 42393) matches (2, 1, 42393)\n",
      "\t\t-[x] values not close enough, max diff: 5.340576171875e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.1444091796875e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.3902783393859863e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.3947486877441406e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.1906027793884277e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[x] values not close enough, max diff: 1.4126300811767578e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 5.340576171875e-05\n",
      "- present.19.key: max diff = 1.1444091796875e-05\n",
      "- present.20.key: max diff = 1.3902783393859863e-05\n",
      "- present.21.key: max diff = 1.3947486877441406e-05\n",
      "- present.22.value: max diff = 1.1906027793884277e-05\n",
      "- present.23.key: max diff = 1.4126300811767578e-05.\n",
      " The exported model was saved at: /Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past\n"
     ]
    }
   ],
   "source": [
    "main_export(\n",
    "model_name_or_path=model_path,\n",
    "task=\"text-generation-with-past\",\n",
    "model_kwargs={\"output_attentions\": True},\n",
    "output=onnx_path.joinpath('bio-gpt-model-with-past'),\n",
    "custom_onnx_configs={\"model\": custom_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use with Optimum libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "model.config.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path  = onnx_path.parent.parent.joinpath('tokenizer_encoder', '1')\n",
    "decoder_path  = onnx_path.parent.parent.joinpath('tokenizer_decoder', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model converted to onnx, we will move to the next step which is to perform quantization on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be exploring quantization approaches to reduce the size of the model and improve the latency for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources: \n",
    "\n",
    "- https://www.philschmid.de/static-quantization-optimum.\n",
    "- https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n",
    "- https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "Quantization is a technique to reduce the the size of neural networks by using lower precision datatype to represent the weight and activation function in the neural network. In general weights and activation are represented as 32-bit floating points, but with quantization we can represent those floating points as 16-bit floating point or sometime using int16 or int8.\n",
    "\n",
    "Quantization have proven to reduce the size of language model hence the inference latency by half while keeping a huge percentage of model accuracy for some downstream tasks. [Source](https://www.philschmid.de/static-quantization-optimum).\n",
    "\n",
    "The bellow image illustrates the effect of the size and inference of quantization on a BERT model.\n",
    "\n",
    "\n",
    "We can see that the model size and the inference time is reduce by third size using 8 bit quantization while the performance of the model remain the same.\n",
    "\n",
    "Quantization does not always keep the same accuracy of the model, so before choosing it we need to make sure we evaluate the performance of the model on the whole dataset.\n",
    "\n",
    "![image](./images/quantization.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we will convert 32 bits floating points to 16 bits, using the onnx library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.config, \"num_attention_heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model =  optimizer.optimize_model(onnx_path.joinpath('bio-gpt-model.onnx').__str__(), \n",
    "                                            model_type='gpt2', \n",
    "                                            num_heads=model.config.num_attention_heads,\n",
    "                                            hidden_size=model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.convert_float_to_float16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = model_path.parent.joinpath(\n",
    "    'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save_model_to_file(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_path.parent.glob(\"*.onnx\"):\n",
    "    print(f\"the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the size of our model have been reduced by 50% using the conversion of floats32 to float 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with this approach that we applied dynamic quantization of the model and it reduce the size of the model! However we could also aplly dynamic quantization to the model but I haven't yet learned about it.  But in [this blog](https://www.philschmid.de/static-quantization-optimum) it have been shown that static quantization improve the inference of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models', 'onnx', 'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_path.parent,\n",
    "                                                      decoder_file_name=model_path,\n",
    "                                                      use_cache=False,\n",
    "                                                      use_io_binding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"question: Is cytokeratin immunoreactivity useful in the diagnosis of short-segment Barrett's oesophagus in Korea? context: Cytokeratin 7/20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7/20 immunostaining for short-segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7/20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7/20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7/20 pattern was observed in 28 out of 36 cases (77.8%) with short-segment Barrett's oesophagus, 11 out of 28 cases (39.3%) with intestinal metaplasia at the cardia, and nine out of 61 cases (14.8%) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7/20 pattern were 77.8 and 77.5%, respectively. answer: Barrett's cytokeratin 7/20 pattern can be a useful marker for the diagnosis of short-segment Barrett's oesophagus, although the false positive or false negative rate is approximately 25%.\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with torch.no_grad():\n",
    "    generated_text = model.generate(**encoded_input,\n",
    "                                min_length=50,\n",
    "                                max_length=1024,\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_text[0], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting GPT2 to ONNX with Beam Search\n",
    "\n",
    "I have found a way to convert the gpt model to Onnx with the support of beam search.\n",
    "\n",
    "I will be using it tomorrow.\n",
    "\n",
    "https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_generation.py#L81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.convert_generation  import convert_generation_model, parse_arguments, GenerationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository',\n",
    "                                \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past = onnx_path.joinpath('bio-gpt-model-with-past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_with_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Namespace(\n",
    "    model_name_or_path=model_path.__str__(),\n",
    "    decoder_onnx=onnx_model_with_past.joinpath(\"model.onnx\").__str__(),\n",
    "    output=onnx_model_with_past.parent.joinpath(\"biogpt-model-with-past-and-beam.onnx\").__str__(),\n",
    "    model_type=\"gpt2\",\n",
    "    num_beams=\"5\",\n",
    "    temperature=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_with_past.joinpath(\"model.onnx\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list = []\n",
    "\n",
    "for key, value in vars(arguments).items():\n",
    "    arguments_list.append(f\"--{key}\")\n",
    "    arguments_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--model_name_or_path',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa',\n",
       " '--decoder_onnx',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx',\n",
       " '--output',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam',\n",
       " '--model_type',\n",
       " 'gpt2',\n",
       " '--num_beams',\n",
       " '5',\n",
       " '--temperature',\n",
       " '0.25']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(arguments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(model_name_or_path='/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa', model_type='gpt2', cache_dir='./cache_models', decoder_onnx='/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx', encoder_decoder_init_onnx='', verbose=False, output='/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam', precision=<Precision.FLOAT32: 'fp32'>, op_block_list=['auto'], use_external_data_format=False, run_shape_inference=False, disable_pad_vocab_size=False, disable_separate_gpt2_decoder_for_init_run=False, disable_shared_initializers=False, output_sequences_scores=False, output_token_scores=False, early_stopping=False, no_repeat_ngram_size=0, vocab_mask=False, past_present_share_buffer=False, use_decoder_masked_attention=False, prefix_vocab_mask=False, custom_attention_mask=False, presence_mask=False, seed=False, min_length=1, max_length=50, num_beams=5, num_return_sequences=1, length_penalty=1, repetition_penalty=1, temperature=0.25, top_p=1.0, filter_value=-inf, min_tokens_to_keep=1, presence_penalty=0.0, custom=0, vocab_size=-1, eos_token_id=-1, pad_token_id=-1, use_sln_strict_mode=False, use_gpu=False, disable_parity=False, disable_perf_test=False, torch_performance=False, total_runs=1, save_test_data=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come back to understand the input generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: the issues seems to be the postional embedding that waht we need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is beam search:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tried and failed to generate the init decoder GPT2 model. Performance may be sub-optimal for the initial decoding run\n",
      "You are using a model of type biogpt to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_beamsearch True\n",
      "should go here \n"
     ]
    }
   ],
   "source": [
    "convert_generation_model(args=args, generation_type=GenerationType.BEAMSEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEed to come back and debug this hidden state issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is working with beam search on the onnx runtime, we need to set it up and use it with the triton inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing Inference with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = onnx_model_with_past.parent.joinpath(\n",
    "    \"bio-gpt-model-with-past\").joinpath(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_beam = onnx_model_with_past.parent.joinpath(\n",
    "    \"biogpt-model-with-past-and-beam.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3720917983.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[58], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx()\u001b[0m\n\u001b[0m                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 11:04:53.429840 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/Constant_12_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.429899 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/Shape_4_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477043 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477064 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477069 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477079 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477094 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477104 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477108 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477115 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477122 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477132 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477137 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477144 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477151 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477155 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477159 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477168 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477173 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477181 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477202 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477211 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477218 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477225 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477231 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477236 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477241 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477247 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477256 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477263 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477272 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477276 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477280 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477287 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477296 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477305 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477311 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477317 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477322 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477330 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477335 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477345 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477353 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477358 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477362 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477367 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477373 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477383 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477388 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477397 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477402 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477411 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477415 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477420 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477428 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477434 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477439 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477446 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477451 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477455 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477460 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477467 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477472 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477479 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477485 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477494 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477500 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477505 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477510 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477516 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477521 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477529 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/activation_fn/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477535 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/activation_fn/Constant_2_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.477540 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/activation_fn/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494007 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494020 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494028 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494036 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494043 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494048 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494055 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494060 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494067 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494072 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494076 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494080 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494088 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494094 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494099 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494103 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494107 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494112 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494124 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494129 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494133 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494139 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494145 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494149 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.15/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494153 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494157 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494164 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494168 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494172 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494177 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494183 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494188 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494192 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494196 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494202 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494207 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.19/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494212 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494216 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494221 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.21/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494227 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494231 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494240 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/final_layer_norm/Constant_1_output_0'. It is not used by a"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : subgraph_gpt.cc:131 Validate Invalid GPT-2 subgraph: number of inputs shall be number of outputs plus 2 or 3 (if past_present_share_buffer) or 5 (if past_present_share_buffer and use_decoder_masked_self_attention for BeamSearch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inference_session \u001b[38;5;241m=\u001b[39m \u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_with_beam\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:383\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:435\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    432\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : subgraph_gpt.cc:131 Validate Invalid GPT-2 subgraph: number of inputs shall be number of outputs plus 2 or 3 (if past_present_share_buffer) or 5 (if past_present_share_buffer and use_decoder_masked_self_attention for BeamSearch)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ny node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494244 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494250 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.13/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494255 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494260 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494264 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.10/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494268 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494275 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.12/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494279 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494284 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494288 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494293 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494297 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494304 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494308 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494312 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494317 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.14/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494322 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494331 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494337 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494343 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.20/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494347 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.18/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494352 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494361 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.11/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494366 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494374 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494379 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494383 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494389 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494393 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494398 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494405 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.23/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494410 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.4/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494414 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494419 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494423 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494427 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494434 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.5/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494441 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494445 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494449 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.2/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494456 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494460 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494465 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.17/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494470 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494475 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494479 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494485 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.0/final_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494491 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.9/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494495 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.7/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494501 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.6/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494506 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.22/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494511 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.16/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494516 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.3/final_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494525 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494530 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.1/self_attn_layer_norm/Constant_1_output_0'. It is not used by any node and should be removed from the model.\n",
      "2024-03-03 11:04:53.494534 [W:onnxruntime:, graph.cc:3543 CleanUnusedInitializersAndNodeArgs] Removing initializer '/biogpt/layers.8/self_attn_layer_norm/Constant_output_0'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "inference_session = InferenceSession(model_with_beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with past work, but not the model with beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {'input_ids': torch.tensor([[2,  4617,  2969,    20,  1994,    21,     6,   533,     5,  1181,\n",
    "                                          17270,   927,  1544,    20,     6,   533,     5,  1181, 17270,    21,\n",
    "                                          14,  8493,  2402,   104]]), \n",
    "            'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.get('input_ids').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(\n",
    "    [1, 2], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = (torch.cumsum(attention_mask, dim=1).type_as(\n",
    "    attention_mask) * attention_mask).long() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask.long() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_shape = [1, 16, 1, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.rand(past_shape, dtype=torch.float16, ) * 2.0 - 1.0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
