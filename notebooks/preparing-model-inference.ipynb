{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onnx Model conversion and Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note load the stuff about ONNX runtime from the machine translation tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path.parent.parent.exists(\n",
    "), f\"Model not found at {model_path.parent.parent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\n",
    "\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_path,  local_files_only=True)\n",
    "model = BioGptForCausalLM.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input):\n",
    "    return tokenizer([input],\n",
    "                     return_tensors='pt',\n",
    "                     max_length=1024,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.onnx import FeaturesManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"seq2seq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"'question:what is the cause of covid ? context: the cause of covid is a virus'\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tokens = model.generate(**encoded_input,\n",
    "                                 num_beams=5,\n",
    "                                 do_sample=True,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository', \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'question: what is the cause of covid? context: the cause of covid is a virus' s triangle. The first step is to identify the agent (s) and to understand the mechanism (s) of transmission. The second step is to control the spread of the virus and reduce the impact on the community. To understand the cause of covid and control the spread of the virus. We conducted a review of the literature using the PubMed database. The articles were selected based on their relevance to the topic. The following were identified: Severe Acute Respiratory Syndrome coronavirus 2 (SARSV-2), Middle East respiratory syndrome coronavirus (MERS-CoV), human bocavirus (HBoV), Hendra virus (HeV), Lagos virus (LV), Houston virus (Houston), Severe Acute Respiratory Syndrome coronavirus (SARS-CoV), and Marburg virus (MARV). The cause of covid-19 is not known. It is known to be caused by: SARSV-2, MERS-CoV, HBoV, HeV, LV, Houston, and Houston. The virus can be transmitted via: (a) respiratory secretions, (b) contact with infected persons, (c) fomites, and (d) other means of transmission. It can also be transmitted directly from person to person. The virus can be transmitted by: (a) respiratory secretions, (b) contact with infected persons, (c) fomites, and (d) other means of transmission. It can also be transmitted directly from person to person. <unk><unk><unk><unk><unk><unk><unk><unk><unk>the answer to the question given the context is yes.\n"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(generate_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:471: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:51: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:199: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:206: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:238: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "export(\n",
    "    model,\n",
    "    tuple(encoded_input.values()),\n",
    "    f=onnx_path.joinpath('bio-gpt-model.onnx'),\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}},\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path  = onnx_path.parent.parent.joinpath('tokenizer_encoder', '1')\n",
    "decoder_path  = onnx_path.parent.parent.joinpath('tokenizer_decoder', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/tokenizer_decoder/1/tokenizer_config.json',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/tokenizer_decoder/1/special_tokens_map.json',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/tokenizer_decoder/1/vocab.json',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/tokenizer_decoder/1/merges.txt',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/tokenizer_decoder/1/added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42393"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model converted to onnx, we will move to the next step which is to perform quantization on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be exploring quantization approaches to reduce the size of the model and improve the latency for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources: \n",
    "\n",
    "- https://www.philschmid.de/static-quantization-optimum.\n",
    "- https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n",
    "- https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "Quantization is a technique to reduce the the size of neural networks by using lower precision datatype to represent the weight and activation function in the neural network. In general weights and activation are represented as 32-bit floating points, but with quantization we can represent those floating points as 16-bit floating point or sometime using int16 or int8.\n",
    "\n",
    "Quantization have proven to reduce the size of language model hence the inference latency by half while keeping a huge percentage of model accuracy for some downstream tasks. [Source](https://www.philschmid.de/static-quantization-optimum).\n",
    "\n",
    "The bellow image illustrates the effect of the size and inference of quantization on a BERT model.\n",
    "\n",
    "\n",
    "We can see that the model size and the inference time is reduce by third size using 8 bit quantization while the performance of the model remain the same.\n",
    "\n",
    "Quantization does not always keep the same accuracy of the model, so before choosing it we need to make sure we evaluate the performance of the model on the whole dataset.\n",
    "\n",
    "![image](./images/quantization.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we will convert 32 bits floating points to 16 bits, using the onnx library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(model.config, \"num_attention_heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model =  optimizer.optimize_model(onnx_path.joinpath('bio-gpt-model.onnx').__str__(), \n",
    "                                            model_type='gpt2', \n",
    "                                            num_heads=model.config.num_attention_heads,\n",
    "                                            hidden_size=model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.convert_float_to_float16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = model_path.parent.joinpath(\n",
    "    'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save_model_to_file(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of decoder_model_quantized the model in MB is: 744.6517105102539\n",
      "the size of bio-gpt the model in MB is: 1488.90811252594\n"
     ]
    }
   ],
   "source": [
    "for model in model_path.parent.glob(\"*.onnx\"):\n",
    "    print(f\"the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the size of our model have been reduced by 50% using the conversion of floats32 to float 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with this approach that we applied dynamic quantization of the model and it reduce the size of the model! However we could also aplly dynamic quantization to the model but I haven't yet learned about it.  But in [this blog](https://www.philschmid.de/static-quantization-optimum) it have been shown that static quantization improve the inference of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models', 'onnx', 'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/models/onnx/decoder_model_quantized.onnx ******* the path form dir ***** /Users/esp.py/Projects/Personal/end-to-end-rag/models/onnx/decoder_model_quantized.onnx True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_path.parent,\n",
    "                                                      decoder_file_name=model_path,\n",
    "                                                      use_cache=False,\n",
    "                                                      use_io_binding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"question: Is cytokeratin immunoreactivity useful in the diagnosis of short-segment Barrett's oesophagus in Korea? context: Cytokeratin 7/20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7/20 immunostaining for short-segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7/20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7/20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7/20 pattern was observed in 28 out of 36 cases (77.8%) with short-segment Barrett's oesophagus, 11 out of 28 cases (39.3%) with intestinal metaplasia at the cardia, and nine out of 61 cases (14.8%) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7/20 pattern were 77.8 and 77.5%, respectively. answer: Barrett's cytokeratin 7/20 pattern can be a useful marker for the diagnosis of short-segment Barrett's oesophagus, although the false positive or false negative rate is approximately 25%.\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_text = model.generate(**encoded_input,\n",
    "                                min_length=50,\n",
    "                                max_length=1024,\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the cause of Covid-19? A case report and review of the literature on Covid-19 in patients with chronic kidney disease (CKD) and end-stage renal disease (ESRD) on hemodialysis (HD) and peritoneal dialysis (PD).'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_text[0], skip_special_tokens=True,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
