{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onnx Model conversion and Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note load the stuff about ONNX runtime from the machine translation tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers.convert_graph_to_onnx import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path.parent.parent.exists(\n",
    "), f\"Model not found at {model_path.parent.parent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\n",
    "\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_path,  local_files_only=True)\n",
    "model = BioGptForCausalLM.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input):\n",
    "    return tokenizer([input],\n",
    "                     return_tensors='pt',\n",
    "                     max_length=1024,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.onnx import FeaturesManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"seq2seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"'question:what is the cause of covid ? context: the cause of covid is a virus'\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "generate_tokens = model.generate(**encoded_input,\n",
    "                                 num_beams=5,\n",
    "                                 do_sample=True,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository', \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mgenerate_tokens\u001b[49m[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(generate_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'past' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpast\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'past' is not defined"
     ]
    }
   ],
   "source": [
    "past[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export with Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be impossible, let us use the model optimum libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx.model_configs import GPT2OnnxConfig\n",
    "from typing import Dict, OrderedDict, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "class CustomBioGPTConfig(GPT2OnnxConfig):\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, \n",
    "                 task: str = \"text-generation-with-past\", \n",
    "                 int_dtype: str = \"int32\", \n",
    "                 float_dtype: str = \"fp16\", \n",
    "                 use_past: bool = True, \n",
    "                 use_past_in_inputs: bool = True, \n",
    "                 preprocessors: List[Any] | None = None, legacy: bool = False):\n",
    "        super().__init__(config, task, int_dtype, float_dtype, use_past, use_past_in_inputs, preprocessors, legacy)\n",
    "        print(\"the int dtype is \", int_dtype)\n",
    "        self._config.n_layer = config.num_hidden_layers\n",
    "        self._config.n_head = config.num_attention_heads\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_inputs = {\"input_ids\": {0: \"batch_size\", 1: \"sequence\"}, \n",
    "                        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                        \"position_ids\": {0: \"batch_size\", 1: \"sequence\"}}\n",
    "        \n",
    "        self.add_past_key_values(common_inputs, direction=\"inputs\")\n",
    "        return common_inputs\n",
    "    \n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_outputs = OrderedDict({\"logits\": {0: \"batch_size\", 1: \"sequence\"}})\n",
    "        self.add_past_key_values(common_outputs, direction=\"outputs\")\n",
    "    \n",
    "        return common_outputs\n",
    "    \n",
    "    def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\n",
    "        \"\"\"\n",
    "        Fills `input_or_outputs` mapping with past_key_values dynamic axes considering the direction.\n",
    "\n",
    "        Args:\n",
    "            inputs_or_outputs (`Dict[str, Dict[int, str]]`):\n",
    "                The mapping to fill.\n",
    "            direction (`str`):\n",
    "                either \"inputs\" or \"outputs\", it specifies whether `input_or_outputs` is the input mapping or the\n",
    "                output mapping, this is important for axes naming.\n",
    "        \"\"\"\n",
    "        if direction not in [\"inputs\", \"outputs\"]:\n",
    "            raise ValueError(\n",
    "                f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n",
    "\n",
    "        if direction == \"inputs\":\n",
    "            decoder_sequence_name = \"past_seq_len\"\n",
    "            name = \"past_key_values\"\n",
    "        else:\n",
    "            decoder_sequence_name = \"total_seq_len\"\n",
    "            name = \"present\"\n",
    "\n",
    "        for i in range(self._normalized_config.num_layers):\n",
    "            inputs_or_outputs[f\"{name}.{i}.key\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}\n",
    "            inputs_or_outputs[f\"{name}.{i}.value\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the int dtype is  int32\n"
     ]
    }
   ],
   "source": [
    "custom_config = CustomBioGPTConfig(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not generate dummy input for \"past_0\". Try adding a proper dummy input generator to the model ONNX config.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dummy_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dummy_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/optimum/exporters/onnx/base.py:632\u001b[0m, in \u001b[0;36mOnnxConfigWithPast.generate_dummy_inputs\u001b[0;34m(self, framework, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_was_inserted:\n\u001b[0;32m--> 632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    633\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not generate dummy input for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Try adding a proper dummy input generator to the model ONNX config.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# refer to https://github.com/huggingface/optimum/pull/764\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_past_in_inputs\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPAD_ATTENTION_MASK_TO_PAST\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m ):\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Obtain the past sequence length from the value instead of the key (Bloom).\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not generate dummy input for \"past_0\". Try adding a proper dummy input generator to the model ONNX config."
     ]
    }
   ],
   "source": [
    "dummy_inputs = custom_config.generate_dummy_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to find a way to overwride the mode and att he attenoin id as input to i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 2.2.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:496: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  elif attention_mask.shape[1] != past_key_values_length + input_shape[1]:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:184: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/Users/esp.py/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:223: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the onnx inputs are dict_keys(['input_ids', 'attention_mask', 'position_ids', 'past_key_values.0.key', 'past_key_values.0.value', 'past_key_values.1.key', 'past_key_values.1.value', 'past_key_values.2.key', 'past_key_values.2.value', 'past_key_values.3.key', 'past_key_values.3.value', 'past_key_values.4.key', 'past_key_values.4.value', 'past_key_values.5.key', 'past_key_values.5.value', 'past_key_values.6.key', 'past_key_values.6.value', 'past_key_values.7.key', 'past_key_values.7.value', 'past_key_values.8.key', 'past_key_values.8.value', 'past_key_values.9.key', 'past_key_values.9.value', 'past_key_values.10.key', 'past_key_values.10.value', 'past_key_values.11.key', 'past_key_values.11.value', 'past_key_values.12.key', 'past_key_values.12.value', 'past_key_values.13.key', 'past_key_values.13.value', 'past_key_values.14.key', 'past_key_values.14.value', 'past_key_values.15.key', 'past_key_values.15.value', 'past_key_values.16.key', 'past_key_values.16.value', 'past_key_values.17.key', 'past_key_values.17.value', 'past_key_values.18.key', 'past_key_values.18.value', 'past_key_values.19.key', 'past_key_values.19.value', 'past_key_values.20.key', 'past_key_values.20.value', 'past_key_values.21.key', 'past_key_values.21.value', 'past_key_values.22.key', 'past_key_values.22.value', 'past_key_values.23.key', 'past_key_values.23.value'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-processing the exported models...\n",
      "Weight deduplication check in the ONNX export requires accelerate. Please install accelerate to run it.\n",
      "Validating ONNX model /Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx...\n",
      "\t-[✓] ONNX model output names match reference model (present.10.value, present.16.value, present.13.key, present.23.key, present.5.value, present.3.key, present.6.key, present.16.key, present.8.key, present.0.key, present.6.value, present.5.key, present.15.key, present.17.value, present.11.key, present.15.value, present.18.key, present.4.value, present.8.value, present.1.value, present.3.value, present.20.value, present.22.key, present.4.key, present.12.value, present.17.key, present.10.key, present.12.key, present.21.value, present.2.value, present.0.value, present.9.value, present.19.value, present.23.value, present.7.key, present.1.key, present.2.key, present.22.value, logits, present.9.key, present.14.key, present.11.value, present.18.value, present.21.key, present.14.value, present.20.key, present.13.value, present.7.value, present.19.key)\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (2, 1, 42393) matches (2, 1, 42393)\n",
      "\t\t-[x] values not close enough, max diff: 3.62396240234375e-05 (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.0.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.1.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.2.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.3.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.4.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.5.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.6.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.7.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.8.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.9.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.10.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.11.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.12.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.13.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.14.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.15.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.16.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.17.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.18.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.19.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.20.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.21.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.22.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.key\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "\t- Validating ONNX Model output \"present.23.value\":\n",
      "\t\t-[✓] (2, 16, 17, 64) matches (2, 16, 17, 64)\n",
      "\t\t-[✓] all values close (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 3.62396240234375e-05.\n",
      " The exported model was saved at: /Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past\n"
     ]
    }
   ],
   "source": [
    "main_export(\n",
    "model_name_or_path=model_path,\n",
    "task=\"text-generation-with-past\",\n",
    "model_kwargs={\"output_attentions\": True},\n",
    "output=onnx_path.joinpath('bio-gpt-model-with-past'),\n",
    "custom_onnx_configs={\"model\": custom_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use with Optimum libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "model.config.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path  = onnx_path.parent.parent.joinpath('tokenizer_encoder', '1')\n",
    "decoder_path  = onnx_path.parent.parent.joinpath('tokenizer_decoder', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model converted to onnx, we will move to the next step which is to perform quantization on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be exploring quantization approaches to reduce the size of the model and improve the latency for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources: \n",
    "\n",
    "- https://www.philschmid.de/static-quantization-optimum.\n",
    "- https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n",
    "- https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "Quantization is a technique to reduce the the size of neural networks by using lower precision datatype to represent the weight and activation function in the neural network. In general weights and activation are represented as 32-bit floating points, but with quantization we can represent those floating points as 16-bit floating point or sometime using int16 or int8.\n",
    "\n",
    "Quantization have proven to reduce the size of language model hence the inference latency by half while keeping a huge percentage of model accuracy for some downstream tasks. [Source](https://www.philschmid.de/static-quantization-optimum).\n",
    "\n",
    "The bellow image illustrates the effect of the size and inference of quantization on a BERT model.\n",
    "\n",
    "\n",
    "We can see that the model size and the inference time is reduce by third size using 8 bit quantization while the performance of the model remain the same.\n",
    "\n",
    "Quantization does not always keep the same accuracy of the model, so before choosing it we need to make sure we evaluate the performance of the model on the whole dataset.\n",
    "\n",
    "![image](./images/quantization.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we will convert 32 bits floating points to 16 bits, using the onnx library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(model.config, \"num_attention_heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model =  optimizer.optimize_model(onnx_path.joinpath('bio-gpt-model.onnx').__str__(), \n",
    "                                            model_type='gpt2', \n",
    "                                            num_heads=model.config.num_attention_heads,\n",
    "                                            hidden_size=model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.convert_float_to_float16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = model_path.parent.joinpath(\n",
    "    'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save_model_to_file(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_path.parent.glob(\"*.onnx\"):\n",
    "    print(f\"the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the size of our model have been reduced by 50% using the conversion of floats32 to float 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with this approach that we applied dynamic quantization of the model and it reduce the size of the model! However we could also aplly dynamic quantization to the model but I haven't yet learned about it.  But in [this blog](https://www.philschmid.de/static-quantization-optimum) it have been shown that static quantization improve the inference of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models', 'onnx', 'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_path.parent,\n",
    "                                                      decoder_file_name=model_path,\n",
    "                                                      use_cache=False,\n",
    "                                                      use_io_binding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = f\"question: Is cytokeratin immunoreactivity useful in the diagnosis of short-segment Barrett's oesophagus in Korea? context: Cytokeratin 7/20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7/20 immunostaining for short-segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7/20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7/20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7/20 pattern was observed in 28 out of 36 cases (77.8%) with short-segment Barrett's oesophagus, 11 out of 28 cases (39.3%) with intestinal metaplasia at the cardia, and nine out of 61 cases (14.8%) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7/20 pattern were 77.8 and 77.5%, respectively. answer: Barrett's cytokeratin 7/20 pattern can be a useful marker for the diagnosis of short-segment Barrett's oesophagus, although the false positive or false negative rate is approximately 25%.\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with torch.no_grad():\n",
    "    generated_text = model.generate(**encoded_input,\n",
    "                                min_length=50,\n",
    "                                max_length=1024,\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_text[0], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting GPT2 to ONNX with Beam Search\n",
    "\n",
    "I have found a way to convert the gpt model to Onnx with the support of beam search.\n",
    "\n",
    "I will be using it tomorrow.\n",
    "\n",
    "https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_generation.py#L81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.convert_generation  import convert_generation_model, parse_arguments, GenerationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository',\n",
    "                                \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past = onnx_path.joinpath('bio-gpt-model-with-past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_with_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Namespace(\n",
    "    model_name_or_path=model_path.__str__(),\n",
    "    decoder_onnx=onnx_model_with_past.joinpath(\"model.onnx\").__str__(),\n",
    "    output=onnx_model_with_past.parent.joinpath(\"biogpt-model-with-past-and-beam.onnx\").__str__(),\n",
    "    model_type=\"gpt2\",\n",
    "    num_beams=\"5\",\n",
    "    temperature=\"0.25\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_with_past.joinpath(\"model.onnx\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list = []\n",
    "\n",
    "for key, value in vars(arguments).items():\n",
    "    arguments_list.append(f\"--{key}\")\n",
    "    arguments_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--model_name_or_path',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa',\n",
       " '--decoder_onnx',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx',\n",
       " '--output',\n",
       " '/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam.onnx',\n",
       " '--model_type',\n",
       " 'gpt2',\n",
       " '--num_beams',\n",
       " '5',\n",
       " '--temperature',\n",
       " '0.25']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(arguments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(model_name_or_path='/Users/esp.py/Projects/Personal/end-to-end-rag/models/bio-gpt-qa', model_type='gpt2', cache_dir='./cache_models', decoder_onnx='/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx', encoder_decoder_init_onnx='', verbose=False, output='/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/biogpt-model-with-past-and-beam.onnx', precision=<Precision.FLOAT32: 'fp32'>, op_block_list=['auto'], use_external_data_format=False, run_shape_inference=False, disable_pad_vocab_size=False, disable_separate_gpt2_decoder_for_init_run=False, disable_shared_initializers=False, output_sequences_scores=False, output_token_scores=False, early_stopping=False, no_repeat_ngram_size=0, vocab_mask=False, past_present_share_buffer=False, use_decoder_masked_attention=False, prefix_vocab_mask=False, custom_attention_mask=False, presence_mask=False, seed=False, min_length=1, max_length=50, num_beams=5, num_return_sequences=1, length_penalty=1, repetition_penalty=1, temperature=0.25, top_p=1.0, filter_value=-inf, min_tokens_to_keep=1, presence_penalty=0.0, custom=0, vocab_size=-1, eos_token_id=-1, pad_token_id=-1, use_sln_strict_mode=False, use_gpu=False, disable_parity=False, disable_perf_test=False, torch_performance=False, total_runs=1, save_test_data=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come back to understand the input generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: the issues seems to be the postional embedding that waht we need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type biogpt to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"input_ids\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 7\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"attention_mask\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 7\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence_length\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"position_ids\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 7\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.0.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.0.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.1.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.1.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.2.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.2.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.3.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.3.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.4.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.4.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.5.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.5.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.6.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.6.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.7.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.7.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.8.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.8.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.9.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.9.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.10.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.10.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.11.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.11.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.12.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.12.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.13.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.13.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.14.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.14.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.15.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.15.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.16.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.16.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.17.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.17.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.18.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.18.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.19.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.19.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.20.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.20.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.21.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.21.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.22.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.22.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.23.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.23.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch_size\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 16\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_seq_len\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is expected to have onnx data type 6. Got 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_generation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGenerationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBEAMSEARCH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/onnxruntime/transformers/convert_generation.py:1826\u001b[0m, in \u001b[0;36mconvert_generation_model\u001b[0;34m(args, generation_type)\u001b[0m\n\u001b[1;32m   1824\u001b[0m gpt2_init_decoder_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1826\u001b[0m     \u001b[43mverify_gpt2_subgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1828\u001b[0m     \u001b[38;5;66;03m# If we generated the init decoder model, verify that as well\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gpt2_init_decoder_generated:\n",
      "File \u001b[0;32m~/Projects/Personal/end-to-end-rag/.venv/lib/python3.10/site-packages/onnxruntime/transformers/convert_generation.py:722\u001b[0m, in \u001b[0;36mverify_gpt2_subgraph\u001b[0;34m(graph, precision)\u001b[0m\n\u001b[1;32m    720\u001b[0m     input_type \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39minput[i]\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mtensor_type\u001b[38;5;241m.\u001b[39melem_type\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_type \u001b[38;5;241m!=\u001b[39m expected_type:\n\u001b[0;32m--> 722\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is expected to have onnx data type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    723\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerifying GPT-2 graph inputs: name and data type are good.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    725\u001b[0m expected_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layer_count)]\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is expected to have onnx data type 6. Got 7"
     ]
    }
   ],
   "source": [
    "convert_generation_model(args=args, generation_type=GenerationType.BEAMSEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEed to come back and debug this hidden state issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is working with beam search on the onnx runtime, we need to set it up and use it with the triton inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing Inference with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = onnx_model_with_past.parent.joinpath(\n",
    "    \"bio-gpt-model-with-past\").joinpath(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_beam = onnx_model_with_past.parent.joinpath(\n",
    "    \"biogpt-model-with-past-and-beam.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_session = InferenceSession(model_with_beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with past work, but not the model with beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {'input_ids': torch.tensor([[2,  4617,  2969,    20,  1994,    21,     6,   533,     5,  1181,\n",
    "                                          17270,   927,  1544,    20,     6,   533,     5,  1181, 17270,    21,\n",
    "                                          14,  8493,  2402,   104]]), \n",
    "            'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.get('input_ids').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(\n",
    "    [1, 2], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = (torch.cumsum(attention_mask, dim=1).type_as(\n",
    "    attention_mask) * attention_mask).long() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask.long() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_shape = [1, 16, 1, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.rand(past_shape, dtype=torch.float16, ) * 2.0 - 1.0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after hacky ways to make the beam search work, we need to make sure the validation of the mode pass.\n",
    "\n",
    "We willl save the attention mask and positions_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
