{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onnx Model conversion and Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note load the stuff about ONNX runtime from the machine translation tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path.parent.parent.exists(\n",
    "), f\"Model not found at {model_path.parent.parent}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptForCausalLM, BioGptTokenizer, set_seed\n",
    "\n",
    "tokenizer = BioGptTokenizer.from_pretrained(model_path,  local_files_only=True)\n",
    "model = BioGptForCausalLM.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input):\n",
    "    return tokenizer([input],\n",
    "                     return_tensors='pt',\n",
    "                     max_length=1024,\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"seq2seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"'question:what is the cause of covid ? context: the cause of covid is a virus'\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "generate_tokens = model.generate(**encoded_input,\n",
    "                                 num_beams=5,\n",
    "                                 do_sample=True,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository', \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generate_tokens[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export with Optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be impossible, let us use the model optimum libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.onnx import main_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, OrderedDict\n",
    "\n",
    "from optimum.exporters.onnx.model_configs import GPT2OnnxConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "class CustomBioGPTConfig(GPT2OnnxConfig):\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig,\n",
    "                 task: str = \"text-generation-with-past\",\n",
    "                 int_dtype: str = \"int32\",\n",
    "                 float_dtype: str = \"fp16\",\n",
    "                 use_past: bool = True,\n",
    "                 use_past_in_inputs: bool = True,\n",
    "                 preprocessors: List[Any] | None = None, legacy: bool = False):\n",
    "        super().__init__(config, task, int_dtype, float_dtype, use_past, use_past_in_inputs, preprocessors, legacy)\n",
    "        print(\"the int dtype is \", int_dtype)\n",
    "        self._config.n_layer = config.num_hidden_layers\n",
    "        self._config.n_head = config.num_attention_heads\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_inputs = {\"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "                        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                        \"position_ids\": {0: \"batch_size\", 1: \"sequence\"}}\n",
    "\n",
    "        self.add_past_key_values(common_inputs, direction=\"inputs\")\n",
    "        return common_inputs\n",
    "\n",
    "    @property\n",
    "    def outputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        common_outputs = OrderedDict({\"logits\": {0: \"batch_size\", 1: \"sequence\"}})\n",
    "        self.add_past_key_values(common_outputs, direction=\"outputs\")\n",
    "\n",
    "        return common_outputs\n",
    "\n",
    "    def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):\n",
    "        \"\"\"\n",
    "        Fills `input_or_outputs` mapping with past_key_values dynamic axes considering the direction.\n",
    "\n",
    "        Args:\n",
    "            inputs_or_outputs (`Dict[str, Dict[int, str]]`):\n",
    "                The mapping to fill.\n",
    "            direction (`str`):\n",
    "                either \"inputs\" or \"outputs\", it specifies whether `input_or_outputs` is the input mapping or the\n",
    "                output mapping, this is important for axes naming.\n",
    "        \"\"\"\n",
    "        if direction not in [\"inputs\", \"outputs\"]:\n",
    "            raise ValueError(\n",
    "                f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n",
    "\n",
    "        if direction == \"inputs\":\n",
    "            decoder_sequence_name = \"past_seq_len\"\n",
    "            name = \"past_key_values\"\n",
    "        else:\n",
    "            decoder_sequence_name = \"total_seq_len\"\n",
    "            name = \"present\"\n",
    "\n",
    "        for i in range(self._normalized_config.num_layers):\n",
    "            inputs_or_outputs[f\"{name}.{i}.key\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}\n",
    "            inputs_or_outputs[f\"{name}.{i}.value\"] = {\n",
    "                0: \"batch_size\", 3: decoder_sequence_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = CustomBioGPTConfig(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = custom_config.generate_dummy_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to find a way to overwride the mode and att he attenoin id as input to i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_export(\n",
    "model_name_or_path=model_path,\n",
    "task=\"text-generation-with-past\",\n",
    "model_kwargs={\"output_attentions\": True},\n",
    "output=onnx_path.joinpath('bio-gpt-model-with-past'),\n",
    "custom_onnx_configs={\"model\": custom_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use with Optimum libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "model.config.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path  = onnx_path.parent.parent.joinpath('tokenizer_encoder', '1')\n",
    "decoder_path  = onnx_path.parent.parent.joinpath('tokenizer_decoder', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(encoder_path)\n",
    "tokenizer.save_pretrained(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model converted to onnx, we will move to the next step which is to perform quantization on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step will be exploring quantization approaches to reduce the size of the model and improve the latency for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources: \n",
    "\n",
    "- https://www.philschmid.de/static-quantization-optimum.\n",
    "- https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\n",
    "- https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "Quantization is a technique to reduce the the size of neural networks by using lower precision datatype to represent the weight and activation function in the neural network. In general weights and activation are represented as 32-bit floating points, but with quantization we can represent those floating points as 16-bit floating point or sometime using int16 or int8.\n",
    "\n",
    "Quantization have proven to reduce the size of language model hence the inference latency by half while keeping a huge percentage of model accuracy for some downstream tasks. [Source](https://www.philschmid.de/static-quantization-optimum).\n",
    "\n",
    "The bellow image illustrates the effect of the size and inference of quantization on a BERT model.\n",
    "\n",
    "\n",
    "We can see that the model size and the inference time is reduce by third size using 8 bit quantization while the performance of the model remain the same.\n",
    "\n",
    "Quantization does not always keep the same accuracy of the model, so before choosing it we need to make sure we evaluate the performance of the model on the whole dataset.\n",
    "\n",
    "![image](./images/quantization.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we will convert 32 bits floating points to 16 bits, using the onnx library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers import optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model =  optimizer.optimize_model(onnx_path.joinpath('bio-gpt-model.onnx').__str__(),\n",
    "                                            model_type='gpt2',\n",
    "                                            num_heads=model.config.num_attention_heads,\n",
    "                                            hidden_size=model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.convert_float_to_float16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = model_path.parent.joinpath(\n",
    "    'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save_model_to_file(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_path.parent.glob(\"*.onnx\"):\n",
    "    print(f\"the size of {model.stem} the model in MB is: {model.stat().st_size / (1024 * 1024)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the size of our model have been reduced by 50% using the conversion of floats32 to float 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with this approach that we applied dynamic quantization of the model and it reduce the size of the model! However we could also aplly dynamic quantization to the model but I haven't yet learned about it.  But in [this blog](https://www.philschmid.de/static-quantization-optimum) it have been shown that static quantization improve the inference of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('models', 'onnx', 'decoder_model_quantized.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_path.parent,\n",
    "                                                      decoder_file_name=model_path,\n",
    "                                                      use_cache=False,\n",
    "                                                      use_io_binding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"question: Is cytokeratin immunoreactivity useful in the diagnosis of short-segment Barrett's oesophagus in Korea? context: Cytokeratin 7/20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7/20 immunostaining for short-segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7/20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7/20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7/20 pattern was observed in 28 out of 36 cases (77.8%) with short-segment Barrett's oesophagus, 11 out of 28 cases (39.3%) with intestinal metaplasia at the cardia, and nine out of 61 cases (14.8%) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7/20 pattern were 77.8 and 77.5%, respectively. answer: Barrett's cytokeratin 7/20 pattern can be a useful marker for the diagnosis of short-segment Barrett's oesophagus, although the false positive or false negative rate is approximately 25%.\"\n",
    "encoded_input = tokenizer([input],\n",
    "                          return_tensors='pt',\n",
    "                          max_length=1024,\n",
    "                          truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with torch.no_grad():\n",
    "    generated_text = model.generate(**encoded_input,\n",
    "                                min_length=50,\n",
    "                                max_length=1024,\n",
    "                                num_beams=5,\n",
    "                                early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_text[0], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Converting GPT2 to ONNX with Beam Search\n",
    "\n",
    "I have found a way to convert the gpt model to Onnx with the support of beam search.\n",
    "\n",
    "I will be using it tomorrow.\n",
    "\n",
    "https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/convert_generation.py#L81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.transformers.convert_generation import (\n",
    "    GenerationType,\n",
    "    convert_generation_model,\n",
    "    parse_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.cwd().joinpath('models')\n",
    "model_id = 'bio-gpt-qa'\n",
    "model_path = model_path.joinpath(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = Path.cwd().joinpath('models_repository',\n",
    "                                \"generator\", \"generator_model\", \"1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past = onnx_path.joinpath('bio-gpt-model-with-past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Namespace(\n",
    "    model_name_or_path=model_path.__str__(),\n",
    "    output=onnx_model_with_past.parent.joinpath(\"biogpt-model-with-past-and-beam\").__str__(),\n",
    "    model_type=\"gpt2\",\n",
    "    num_beams=\"5\",\n",
    "    temperature=\"0.25\",\n",
    "    model_class=\"BioGptModel\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_with_past.joinpath(\"model.onnx\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list = []\n",
    "\n",
    "for key, value in vars(arguments).items():\n",
    "    arguments_list.append(f\"--{key}\")\n",
    "    arguments_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(arguments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come back to understand the input generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: the issues seems to be the postional embedding that waht we need to fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_generation_model(args=args, generation_type=GenerationType.BEAMSEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEed to come back and debug this hidden state issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is working with beam search on the onnx runtime, we need to set it up and use it with the triton inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing Inference with the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to come to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = onnx_model_with_past.parent.joinpath(\n",
    "    \"bio-gpt-model-with-past\").joinpath(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_beam = onnx_model_with_past.parent.joinpath(\n",
    "    \"biogpt-model-with-past-and-beam.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import InferenceSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/esp.py/Projects/Personal/end-to-end-rag/models_repository/generator/generator_model/1/bio-gpt-model-with-past/model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_session = InferenceSession(model_with_beam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with past work, but not the model with beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {'input_ids': torch.tensor([[2,  4617,  2969,    20,  1994,    21,     6,   533,     5,  1181,\n",
    "                                          17270,   927,  1544,    20,     6,   533,     5,  1181, 17270,    21,\n",
    "                                          14,  8493,  2402,   104]]),\n",
    "            'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.get('input_ids').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(\n",
    "    [1, 2], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = (torch.cumsum(attention_mask, dim=1).type_as(\n",
    "    attention_mask) * attention_mask).long() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask.long() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_shape = [1, 16, 1, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.rand(past_shape, dtype=torch.float16, ) * 2.0 - 1.0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after hacky ways to make the beam search work, we need to make sure the validation of the mode pass.\n",
    "\n",
    "We willl save the attention mask and positions_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
