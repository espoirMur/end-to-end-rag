{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ff5b0d-2052-48fc-895d-1b02f8d4e6ee",
   "metadata": {},
   "source": [
    "### Loading the model checkpoint using fairseq extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1080005-3f03-4077-966a-5b59ec95c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0922cd-a772-467f-a3ca-1211ce39eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_checkpoint_path = Path.cwd().joinpath(\n",
    "    'models', 'QA-PubMedQA-BioGPT', \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961dfe1-67bf-4b7a-a233-9e868c3d7268",
   "metadata": {},
   "source": [
    "After looking at the architecture, let us load the model directly using fairseq and later we will decide to load it using transformers and may be transform it to huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55fe52-91c1-4fd8-aaed-21e54e588cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778829e-d9c4-44ff-99e9-722086e4ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biogpt_model.transformer_lm_prompt import TransformerLanguageModelPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd464b5-675c-44b8-9487-11b4463939dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().joinpath('datasets', 'biogpt', 'pqal_qcl_ansis-bin')\n",
    "bpe_code_path = data_path.parent.joinpath('raw', 'bpecodes')\n",
    "assert data_path.exists()\n",
    "assert bpe_code_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d91dcc-b608-4eb6-b23a-61cee0e1add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fairseq = TransformerLanguageModelPrompt.from_pretrained(\n",
    "    qa_model_checkpoint_path.parent,\n",
    "    \"checkpoint.pt\",\n",
    "    data_path.__str__(),\n",
    "    tokenizer=\"moses\",\n",
    "    bpe=\"fastbpe\",\n",
    "    bpe_codes=bpe_code_path.__str__(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcb8b1-cedb-4d09-b74e-196fee89c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fairseq.cfg.get(\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabbce4-c29a-4adc-80f3-54f9b9a68dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c824f4-a7aa-4084-b45b-ee73c315e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [\n",
    "    'Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.',\n",
    "    'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea34c64-2d8b-4f2a-95a2-e1ea6e1df9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381721da-39a5-4a5e-bc84-ed813b8d3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"question: {question} context: { ' '.join(contexts)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe14d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2f003-fb2b-4fda-8a8c-6983ad39f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokens  = model_fairseq.encode(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618720",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb42981",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c6453-8914-4b65-a39a-d26944b19b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = model_fairseq.generate([source_tokens],\n",
    "                                        beam=5,\n",
    "                                        min_len=100,\n",
    "                                        max_len_a=512,\n",
    "                                        max_len_b=512,\n",
    "                                        temperature=0.25,\n",
    "                                        sampling_topk=50,\n",
    "                                        sampling_topp=0.95,\n",
    "                                        sampling=True,)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321f398-6c77-4f9a-b8d2-2cf53f7c23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fairseq.max_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117e20f-3d97-46e9-9367-b1351e7e475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(source_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966f081-480b-4cdf-9e02-4c10a9d0aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dc0ad-201d-4b7d-ad84-7d4155242d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = model_fairseq.decode(generated_text[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac969bd-b6aa-4491-bf61-3781c627995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a445f5-c36e-40bd-a200-660d9fe59cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa993f-3210-4e22-949b-385ecaf73ecc",
   "metadata": {},
   "source": [
    "The prompt model seems to be working but always returning learned. What am I missing here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e87eea-a2f4-4118-89df-6fe51c7877dc",
   "metadata": {},
   "source": [
    "Not sure why the model generated this type of data, that may be because of the data. But it worth checking what went wrong with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4975058",
   "metadata": {},
   "source": [
    "### Uploading the model to hugginface transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca174b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fairseq.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964ca0e",
   "metadata": {},
   "source": [
    "This is the model architecture, the next step is to convert the architecture to the huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf8a01",
   "metadata": {},
   "source": [
    "Will start here tommorow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4d346",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9927e87",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980afa63",
   "metadata": {},
   "source": [
    "### Model Conversion to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad422ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4121e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80510b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path.cwd().joinpath('datasets', 'biogpt', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb73b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_path.exists(), \"Model path does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "biogpt_qa_hf_path = Path.cwd().joinpath('models', 'bio-gpt-qa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe26d0",
   "metadata": {},
   "source": [
    "###### Convert me to python cell to execute\n",
    "convert_biogpt_checkpoint_to_pytorch(biogpt_checkpoint_path=model_path, \n",
    "                                     pytorch_dump_folder_path=biogpt_qa_hf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae5fd8",
   "metadata": {},
   "source": [
    "Now we have a problem, the fairseq  model has a vocabulary size  of 42384 while the model embedding layers has a size of 42393 words. It looks like in the embedding layers we have added  9 words which are learned1, learned2, learned3.... and learned9. Those words aret he words that the model is always generating before putting the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d576fc6",
   "metadata": {},
   "source": [
    "Let us see how the model will perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BioGptForCausalLM, BioGptTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361bc3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ba46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BioGptTokenizer.from_pretrained(biogpt_qa_hf_path)\n",
    "bio_gpt_model = BioGptForCausalLM.from_pretrained(biogpt_qa_hf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de656",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_tokens = bio_gpt_model.generate(tokenized_text,\n",
    "                                         num_beams=5,\n",
    "                                         do_sample=True,\n",
    "                                         top_k=50,\n",
    "                                         top_p=0.95,\n",
    "                                         max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52437800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb539e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tokenizer.decode(generate_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfb7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_gpt_model.push_to_hub(\"BioGPT-Large-QA-PubMedQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b5573",
   "metadata": {},
   "source": [
    "This si where we stop today, I will comeback tommorow to learn why the prompt is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"BioGPT-Large-QA-PubMedQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f0433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
