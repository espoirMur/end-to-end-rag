{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implementing the retrieval! We will query our postgres database that has our text and the context. Then we will try to implement ranking capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "database_user = getenv('POSTGRES_USER')\n",
    "database_password = getenv('POSTGRES_PASSWORD')\n",
    "database_host = getenv('POSTGRES_HOST')\n",
    "database_port = getenv('POSTGRES_PORT')\n",
    "database_name = getenv('POSTGRES_DB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_uri = f'postgresql://{database_user}:{quote(database_password)}@{database_host}:{database_port}/{database_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import connect\n",
    "from pgvector.psycopg2 import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection = connect(\n",
    "    user=database_user,\n",
    "    password=database_password,\n",
    "    host=database_host,\n",
    "    port=database_port,\n",
    "    database=database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_connection.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(database_connection, query, params=None) -> Optional[List[Any]]:\n",
    "    with database_connection.cursor() as cursor:\n",
    "        cursor.execute(query, params)\n",
    "        try:\n",
    "            return cursor.fetchall()\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaable accent characters! \n",
    "\n",
    "French is a word with dialetric or accent, my queries didn't work with accented character reason why I had to find a way to remove accent from the characters. To achieve that I used what this guide in [postgres recomend](https://www.postgresql.org/docs/current/unaccent.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(database_connection, \"create extension if not exists unaccent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(database_connection, \"CREATE TEXT SEARCH CONFIGURATION unaccent_french ( COPY = french );\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(database_connection, \"ALTER TEXT SEARCH CONFIGURATION unaccent_french ALTER MAPPING FOR hword, hword_part, word WITH unaccent, french_stem;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Searching and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "random_id = randint(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article = execute_query(database_connection, f'SELECT content FROM article WHERE id = {random_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize as unicode_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_text = unicode_normalize('NFKD', random_article[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions =  [\"Quand l’Ordonnance présidentielle a-t-elle été lue sur le plateau de la Radiotélévision nationale congolaise (RTNC)?\",\n",
    "\"Qui a été nommé pour remplacer Emmanuel Ramazani Shadary au poste de vice-Premier ministre et ministre de l’Intérieur et sécurité?\",\n",
    "\"Où et quand Henri Mova Sakanyi est-il né?\",\n",
    "\"Quelle est la carrière politique de Henri Mova Sakanyi en République démocratique du Congo?\",\n",
    "\"Quel est le poste actuel de Henri Mova Sakanyi au sein du Parti du peuple pour la Reconstruction et la Démocratie (PPRD)?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain as itertools_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(database_connection,\n",
    "              f\"select * from phraseto_tsquery('unaccent_french', 'Quand l’Ordonnance présidentielle a-t-elle été lue sur le plateau de la Radiotélévision nationale congolaise (RTNC)?')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(conn, query: str) -> List[Any]:\n",
    "    model = SentenceTransformer(model_id)\n",
    "    embedding = model.encode(query)\n",
    "    semantic_search_query = 'SELECT id, chunk FROM article_embeddings ORDER BY chunk_vector <=> %(embedding)s LIMIT 5'\n",
    "    results = execute_query(conn, semantic_search_query, {\n",
    "                            'embedding': str(embedding.tolist())})\n",
    "    return results\n",
    "\n",
    "def keyword_search(conn, query: str) -> List[Any]:\n",
    "\n",
    "    keyword_search_query_string = \"\"\"SELECT article_id, chunk \n",
    "                                FROM article_embeddings, websearch_to_tsquery(%(language)s, %(query)s) query\n",
    "                                  WHERE to_tsvector(%(language)s, chunk) @@ query \n",
    "                                ORDER BY ts_rank_cd(to_tsvector(%(language)s, chunk), query) DESC LIMIT %(limit)s;\"\"\"\n",
    "    results = execute_query(conn, keyword_search_query_string, {'language': 'unaccent_french', 'query': query, 'limit': 5})\n",
    "    return results\n",
    "\n",
    "\n",
    "def rerank(query: str, results: List[Tuple[int, str]]) -> List[Any]:\n",
    "    # deduplicate\n",
    "    results = [result[1] for result in results]\n",
    "    results = set(results)\n",
    "    # re-rank\n",
    "    encoder = CrossEncoder(model_id)\n",
    "    scores = encoder.predict([(query, item) for item in results])\n",
    "    return [v for _, v in sorted(zip(scores, results), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    print(f'Question: {question}')\n",
    "    semantic_results = [] # semantic_search(database_connection, question)\n",
    "    keyword_results = keyword_search(database_connection, question)\n",
    "    results = semantic_results + keyword_results\n",
    "    # reranked_results = rerank(question, semantic_results + keyword_results)\n",
    "    for result in results:\n",
    "        print(f'Article: {result[1]}')\n",
    "    print(19 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial to improve keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc = nlp(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(spacy_doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy import extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " extract.keyterms.textrank(spacy_doc, normalize=\"lemma\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improvement on the keyword search will help us to retrieve the top three keywords from a question. Then we will use those questions to preform a keyword search in postgres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_keyword_extraction(text: str) -> str:\n",
    "    \"\"\"This function will perform keyword extraction the text supplied.\n",
    "    It used spacy and texacy and will perform keword exraction and will return those top keywords ready to be used in websearch_text \n",
    "    function.\n",
    "    The keywords will be combined with 'or' operator.\n",
    "    \"\"\"\n",
    "    spacy_doc = nlp(text)\n",
    "    term_keys = extract.keyterms.textrank(spacy_doc, normalize=\"lemma\", topn=3)\n",
    "    return \" or \".join([f'\"{term[0]}\"' for term in term_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    print(f'Question: {question}')\n",
    "    keywords = perform_keyword_extraction(question)\n",
    "    results = keyword_search(database_connection, keywords)\n",
    "    print(f'Keywords: {keywords}')  \n",
    "    print(19 * '-')\n",
    "    for result in results:\n",
    "        print(f'Article: {result[1]}')\n",
    "    print(19 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This where async await code come into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    print(f'Question: {question}')\n",
    "    semantic_results = semantic_search(database_connection, question)\n",
    "    keywords = perform_keyword_extraction(question)\n",
    "    keyword_results = keyword_search(database_connection, keywords)\n",
    "    results = semantic_results + keyword_results\n",
    "    reranked_results = rerank(question, results)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f'Article {i}: {result[1]}')\n",
    "    print(19 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the look at the model, I can see that It nede some finnetuning on the text to generate better results. But that is a step of another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this the retrieval part is completed, the next step will be to use a small model call the fusion in decoder to perform generative question anwering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
